{"cells":[{"cell_type":"markdown","metadata":{"id":"UWiGVj6njoDn"},"source":["# Unsupervised Pre-Training of GPT-Style Model\n","\n","In today's notebook, we'll be working through an example of how to do unsupervised pre-training of a GPT-style model.\n","\n","The base model we'll use is Andrej Karpathy's [nanoGPT](https://github.com/karpathy/nanoGPT).\n","\n","All of the model code can be found in the [`model.py`](https://github.com/karpathy/nanoGPT/blob/master/model.py) file!\n","\n","> NOTE: We will not be leveraging the parallized training strategy in this notebook - you can find all the required code in the provided repository."]},{"cell_type":"markdown","metadata":{"id":"eHi04aEnkKEZ"},"source":["## Data Selection\n","\n","For the notebook today, we'll be using a toy dataset called `tinyshakespeare`. Feel free to use your own corpus here, just make sure it's contained within a single `.txt` file.\n","\n","You could extend this example to use the [OpenWebText](https://skylion007.github.io/OpenWebTextCorpus/) dataset, which was used to pre-train GPT-2.\n","\n","> NOTE: Training LLMs can take a very long time - in order to get results similar to the [GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) you will need 8xA100s and train for ~4-5 days using a pararellized strategy (DDP) on the OpenWebText Corpus.\n","\n","Let's start by grabbing our source repository for the day!"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["'/home/paperspace/llm-engineering-course/hw1'"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["%pwd"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1213,"status":"ok","timestamp":1699406332606,"user":{"displayName":"Neil Tedeschi","userId":"14463069118947243503"},"user_tz":480},"id":"lMRsEQZy6tgc","outputId":"2446fbbe-2f10-418d-ffc3-d4e91b8e0289"},"outputs":[],"source":["# !git clone https://github.com/karpathy/nanoGPT.git"]},{"cell_type":"markdown","metadata":{"id":"6l4CqoEDl7ks"},"source":["Next, we'll need to grab some dependencies.\n","\n","`cohere` and `openai` are recent dependencies of `tiktoken`, but we will not be leveraging them today."]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7015,"status":"ok","timestamp":1699406339605,"user":{"displayName":"Neil Tedeschi","userId":"14463069118947243503"},"user_tz":480},"id":"d_gepPv1Qdj_","outputId":"f3093e8a-9ce5-4d74-c885-f1e924919d15"},"outputs":[],"source":["# %pip install tiktoken requests cohere openai -q"]},{"cell_type":"markdown","metadata":{"id":"70hSjXmZmCt3"},"source":["First things first - let's download our dataset!\n","\n","We'll leverage the `requests` library to do this - and then we will split our resultant data into a `train` and `val` set. We want ~90% of our data to be training, and ~10% to be validation."]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":356,"status":"ok","timestamp":1699406500215,"user":{"displayName":"Neil Tedeschi","userId":"14463069118947243503"},"user_tz":480},"id":"T7qRWArUNiZ5"},"outputs":[],"source":["import os\n","from pathlib import Path\n","import requests\n","import tiktoken\n","import numpy as np\n","\n","# base_path = Path(\"/Users/neil/Projects/llm-engineering-course/hw1\") \n","base_path = Path(\"/home/paperspace/llm-engineering-course/hw1\") \n","data_path = base_path / \"data/shakespeare\"\n","input_file_path = data_path / \"input.txt\""]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1115394\n"]}],"source":["current_path = base_path / \"data/shakespeare\"\n","data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n","\n","if not os.path.exists(current_path):\n","    os.makedirs(current_path)\n","\n","# download the tiny shakespeare dataset\n","# input_file_path = os.path.join(os.path.dirname(current_path), 'input.txt')\n","input_file_path = current_path / \"input.txt\"\n","if not os.path.exists(input_file_path):\n","\n","    with open(input_file_path, 'w') as f:\n","        f.write(requests.get(data_url).text)\n","\n","with open(input_file_path, 'r') as f:\n","    data = f.read()\n","\n","n = len(data)\n","print(n)"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":447,"status":"ok","timestamp":1699406591603,"user":{"displayName":"Neil Tedeschi","userId":"14463069118947243503"},"user_tz":480},"id":"-SK-RQYphWH8"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["i = int(np.ceil(0.9*n))\n","train_data = data[:i]\n","val_data = data[i:]\n","n == len(train_data) + len(val_data)"]},{"cell_type":"markdown","metadata":{"id":"wU9BG2CymU-a"},"source":["Now let's get our `tokenizers` dependency so we can train a tokenizer on our data."]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5478,"status":"ok","timestamp":1699406623785,"user":{"displayName":"Neil Tedeschi","userId":"14463069118947243503"},"user_tz":480},"id":"gFnrwKpQPsYh","outputId":"7550cc2b-c939-4cb0-bd00-29cf0776e982"},"outputs":[],"source":["#%pip install tokenizers -qU"]},{"cell_type":"markdown","metadata":{"id":"rmWXE5ctma9Z"},"source":["We will be training a \"byte-pair-encoding\" or \"BPE\" tokenizer. If you'd like to read more, you can find it [here](https://en.wikipedia.org/wiki/Byte_pair_encoding).\n","\n","Let's work through an example of what Byte-Pair Encoding (BPE) is doing, exactly, from this wonderful example provided by [Hugging Face](https://huggingface.co/docs/transformers/main/tokenizer_summary#byte-pair-encoding-bpe).\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GLecDiHbogvX"},"source":["### What is BPE?\n","\n","First, we need to do a step called \"pre-tokenization\", which is - as it sounds - a tokenization step that occurs before we tokenize.\n","\n","The essential idea of BPE is that we need to understand common words and \"byte-pairs\" in them. So, in order to find \"common words\" we first need to find...words!\n","\n","Let's take the following text and break it apart into its word components.\n","\n","\n","```\n","After pre-tokenization, a set of unique words has been created and the frequency with which each word occurred in the training data has been determined. Next, BPE creates a base vocabulary consisting of all symbols that occur in the set of unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. It does so until the vocabulary has attained the desired vocabulary size. Note that the desired vocabulary size is a hyperparameter to define before training the tokenizer.\n","```\n","\n","A naive way to do this would just be by splitting on spaces...and that is indeed what technique was used in GPT-2."]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":350,"status":"ok","timestamp":1699406691660,"user":{"displayName":"Neil Tedeschi","userId":"14463069118947243503"},"user_tz":480},"id":"m34NDAGCpiz6"},"outputs":[],"source":["input_text = \"\"\"\n","After pre-tokenization, a set of unique words has been created and the frequency with which each word occurred in the training data has been determined. Next, BPE creates a base vocabulary consisting of all symbols that occur in the set of unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. It does so until the vocabulary has attained the desired vocabulary size. Note that the desired vocabulary size is a hyperparameter to define before training the tokenizer.\n","\"\"\"\n","\n","naive_word_list = input_text.split()"]},{"cell_type":"markdown","metadata":{"id":"hR8k-2bopqjy"},"source":["Now we can count our words and get their frequency."]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1699406700421,"user":{"displayName":"Neil Tedeschi","userId":"14463069118947243503"},"user_tz":480},"id":"B_201bSQpvqD","outputId":"4a33c150-6327-4365-9cd9-ebeb69c66219"},"outputs":[{"data":{"text/plain":["[('t h e', 8), ('a', 4), ('o f', 4), ('v o c a b u l a r y', 4), ('h a s', 3)]"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["from collections import defaultdict\n","\n","vocab_and_frequencies = defaultdict(int)\n","\n","for word in naive_word_list:\n","  vocab_and_frequencies[\" \".join(list(word))] += 1\n","\n","sorted(vocab_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"]},{"cell_type":"markdown","metadata":{"id":"NckufSxxp-w5"},"source":["Let's find our \"base vocabulary\", which is going to be each symbol present in our original dataset."]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":381,"status":"ok","timestamp":1699406715187,"user":{"displayName":"Neil Tedeschi","userId":"14463069118947243503"},"user_tz":480},"id":"BNcjzjDvvKjp"},"outputs":[],"source":["from typing import Dict, Tuple, List, Set\n","\n","def find_vocabulary_size(current_vocab: Dict[str, int]) -> int:\n","  vocab = set()\n","\n","  for word in current_vocab.keys():\n","    for subword in word.split():\n","      vocab.add(subword)\n","\n","  return len(vocab)"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1699406716356,"user":{"displayName":"Neil Tedeschi","userId":"14463069118947243503"},"user_tz":480},"id":"pf3kCf-WvdBL","outputId":"0d213350-4b63-41ca-acae-327cc28c29ef"},"outputs":[{"data":{"text/plain":["34"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["find_vocabulary_size(vocab_and_frequencies)"]},{"cell_type":"markdown","metadata":{"id":"VoMq7GhKqf7p"},"source":["As we can see, there are ~34 symbols in our base vocabulary. Let's convert our data into a form where we can capture each symbol separately."]},{"cell_type":"markdown","metadata":{"id":"OGxrHYmftDTr"},"source":["Now we can start constructing our pairs. We will look at all the pairs of symbols as they appear and take into consideration their frequency in our corpus."]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":333,"status":"ok","timestamp":1699406741086,"user":{"displayName":"Neil Tedeschi","userId":"14463069118947243503"},"user_tz":480},"id":"sTwvfTAErQN7"},"outputs":[],"source":["def find_pairs_and_frequencies(current_vocab: Dict[str, int]) -> Dict[str, int]:\n","  pairs = {}\n","\n","  for word, frequency in current_vocab.items():\n","    symbols = word.split()\n","\n","    for i in range(len(symbols) - 1):\n","      pair = (symbols[i], symbols[i + 1])\n","      current_frequency = pairs.get(pair, 0)\n","      pairs[pair] = current_frequency + frequency\n","\n","  return pairs"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":447,"status":"ok","timestamp":1699406743360,"user":{"displayName":"Neil Tedeschi","userId":"14463069118947243503"},"user_tz":480},"id":"FudOaKmYv9-y"},"outputs":[],"source":["pairs_and_frequencies = find_pairs_and_frequencies(vocab_and_frequencies)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1699406744590,"user":{"displayName":"Neil Tedeschi","userId":"14463069118947243503"},"user_tz":480},"id":"oGIJfkk7wFYw","outputId":"28e2c32d-5d8a-4954-badb-e9e2e1850846"},"outputs":[{"data":{"text/plain":["[(('t', 'h'), 11),\n"," (('i', 'n'), 10),\n"," (('r', 'e'), 8),\n"," (('h', 'e'), 8),\n"," (('a', 't'), 7)]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["sorted(pairs_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"]},{"cell_type":"markdown","metadata":{"id":"OqORqdzwsZ6s"},"source":["Now that we have the frequent pairs - we can merge those pairs into a single token.\n","\n","Let's see how this process looks in code."]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":468,"status":"ok","timestamp":1699406753258,"user":{"displayName":"Neil Tedeschi","userId":"14463069118947243503"},"user_tz":480},"id":"L7ohHm2kshoY"},"outputs":[],"source":["import re\n","\n","def merge_vocab(most_common_pair: Tuple[str], current_vocab: Dict[str, int]) -> Dict[str, int]:\n","  vocab_out = {}\n","\n","  pattern = re.escape(' '.join(most_common_pair))\n","  replacement = ''.join(most_common_pair)\n","\n","  for word_in in current_vocab:\n","      word_out = re.sub(pattern, replacement, word_in)\n","      vocab_out[word_out] = current_vocab[word_in]\n","\n","  return vocab_out"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1699406753719,"user":{"displayName":"Neil Tedeschi","userId":"14463069118947243503"},"user_tz":480},"id":"Ab760KKuwzZ6"},"outputs":[],"source":["new_vocab_and_frequencies = merge_vocab(\n","    sorted(pairs_and_frequencies.items(), key = lambda x: x[1], reverse=True)[0][0],\n","    vocab_and_frequencies\n",")"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1699406756233,"user":{"displayName":"Neil Tedeschi","userId":"14463069118947243503"},"user_tz":480},"id":"L0XtvLbpxbSx","outputId":"38235233-9219-45e5-d430-acb96619d9f4"},"outputs":[{"data":{"text/plain":["[('th e', 8), ('a', 4), ('o f', 4), ('v o c a b u l a r y', 4), ('h a s', 3)]"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["sorted(new_vocab_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"]},{"cell_type":"markdown","metadata":{"id":"9DPkBzj2u-me"},"source":["After one merge, we can see that `t h` has been converted to `th`!\n","\n","Let's see how that impacted our vocabulary."]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":352,"status":"ok","timestamp":1699406783030,"user":{"displayName":"Neil Tedeschi","userId":"14463069118947243503"},"user_tz":480},"id":"bO_xegCtxjQf","outputId":"f0983b23-db12-4b26-a911-3e9497924ab5"},"outputs":[{"data":{"text/plain":["35"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["find_vocabulary_size(new_vocab_and_frequencies)"]},{"cell_type":"markdown","metadata":{"id":"o3M13D60xzZi"},"source":["We can see that our vocabulary has increased by 1 as we've added the `th` symbol to it!\n","\n","In essence, BPE will continue to do this process until your desired vocabulary size (a hyper-parameter) is met!"]},{"cell_type":"markdown","metadata":{"id":"BePYCbHly02H"},"source":["## Training Our Tokenizer\n","\n","Now that we have some background on how BBPE works, lets move on to training our tokenizer for our model!\n","\n","Let's walk through the steps we'll take:\n","\n","1. Initialize our `Tokenizer` with a `BPE` model. Be sure to include the `unk_token`.\n","\n","  - [`Tokenizer`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizer)\n","  - [`Models`](https://huggingface.co/docs/tokenizers/api/models#models)\n","\n","2. We'll include a normalizer, applied at the sequence level, and we'll use `NFD()` to do so. More reading on Unicode Normalization Forms [here](https://unicode.org/reports/tr15/#Normalization_Forms_Table).\n","\n","  - [`NFD()`](https://huggingface.co/docs/tokenizers/api/normalizers#tokenizers.normalizers.NFD)\n","\n","3. We'll also add our `ByteLevel()` pre-tokenizer, and our `ByteLevelDecoder()` decoder.\n","\n","  - [`ByteLevel()`](https://huggingface.co/docs/tokenizers/api/pre-tokenizers#tokenizers.pre_tokenizers.ByteLevel)\n","  - [`ByteLevelDecoder()`](https://huggingface.co/docs/tokenizers/api/decoders#tokenizers.decoders.ByteLevel)"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"OrztE09OPosB"},"outputs":[],"source":["from tokenizers import Tokenizer\n","from tokenizers.models import BPE\n","from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n","from tokenizers.normalizers import NFD, Sequence\n","from tokenizers.trainers import BpeTrainer\n","from tokenizers.pre_tokenizers import ByteLevel\n","\n","tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n","tokenizer.normalizer = Sequence([\n","    NFD()\n","])\n","tokenizer.pre_tokenizer = ByteLevel()\n","tokenizer.decoder = ByteLevelDecoder()"]},{"cell_type":"markdown","metadata":{"id":"dDqkNNdM1KsD"},"source":["We'll want to add some special tokens to our tokenizer to ensure it has access to common token patterns.\n","\n","Let's use the following:\n","\n","- `\"<s>\"`    : bos_token - beginning of sequence token\n","- `\"</s>\"`   : eos_token - end of sequence token\n","- `\"<pad>\"`  : padding_token - token used to pad sequences\n","- `\"<unk>\"`  : unk_token - token used to represent unknown tokens.\n","- `\"<mask>\"` : mask_token - token used to mask parts of our sequence\n","\n","We're also going to set a target vocabulary of 50,000 tokens."]},{"cell_type":"code","execution_count":21,"metadata":{"id":"x9iQVhN3P3RN"},"outputs":[],"source":["trainer = BpeTrainer(\n","    vocab_size=50000,\n","    show_progress=True,\n","    special_tokens=[\n","      \"<unk>\", \"<s>\", \"<pad>\", \"<mask>\", \"</s>\"\n","    ]\n",")"]},{"cell_type":"markdown","metadata":{"id":"yQ8X9vZe2Fyw"},"source":["Nothing left to do but point it at our data-source and let it train!\n","\n","We'll use the `.train()` method to accomplish this task.\n","\n","> NOTE: Pay attention to the desired inputs of the `.train()` method.\n","\n","- [`Tokenizer.train()`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizers.Tokenizer.train)"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"LinLHotSP7gv"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n"]}],"source":["tokenizer.train(\n","    files=[str(input_file_path)],\n",")"]},{"cell_type":"markdown","metadata":{"id":"V2JNYiqB2qKV"},"source":["Now we can save our tokenizer - and then load it as a `GPT2Tokenizer` through the Hugging Face Library!"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":164,"status":"ok","timestamp":1699399263943,"user":{"displayName":"Chris Alexiuk","userId":"06521420203810986327"},"user_tz":300},"id":"Jk6QjDGHQy2K","outputId":"2057465c-ae00-4859-a557-5b5f7e3f4654"},"outputs":[{"data":{"text/plain":["['/home/paperspace/llm-engineering-course/hw1/tokenizer/vocab.json',\n"," '/home/paperspace/llm-engineering-course/hw1/tokenizer/merges.txt']"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["# save_path = '/content/tokenizer'\n","save_path = base_path / 'tokenizer'\n","if not os.path.exists(save_path):\n","    os.makedirs(save_path)\n","tokenizer.model.save(str(save_path))"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10078,"status":"ok","timestamp":1699399274020,"user":{"displayName":"Chris Alexiuk","userId":"06521420203810986327"},"user_tz":300},"id":"cOOlbggdRFrN","outputId":"27299391-0c7e-4404-a116-983bb9335053"},"outputs":[],"source":["#!pip install transformers -qU"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"us1vofdhQ45C"},"outputs":[],"source":["from transformers import GPT2Tokenizer\n","\n","tokenizer = GPT2Tokenizer.from_pretrained(save_path, unk_token=\"[UNK]\")"]},{"cell_type":"markdown","metadata":{"id":"0-Bnq7lV2xWo"},"source":["Let's see how it tokenizes our inputs!"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"dnYnFa3fTRLf"},"outputs":[],"source":["input_sentence = \"Hark, my name be Romeo! I am but a beautiful summer's day!\""]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1699399278473,"user":{"displayName":"Chris Alexiuk","userId":"06521420203810986327"},"user_tz":300},"id":"PSHY5VufRbBj","outputId":"40f4dc82-98a4-40cb-b07e-ec56affa8094"},"outputs":[{"ename":"SyntaxError","evalue":"invalid syntax (1588958192.py, line 1)","output_type":"error","traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[27], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    tokenized_sentence = ### ??\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["tokenized_sentence = ### ??\n","tokenized_sentence"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1699399278473,"user":{"displayName":"Chris Alexiuk","userId":"06521420203810986327"},"user_tz":300},"id":"eZrWzQQlTU41","outputId":"07807b07-bc2f-4d2c-eda8-d544cdaf09e3"},"outputs":[{"data":{"text/plain":["[12072, 4, 119, 632, 116, 821, 0, 82, 290, 214, 67, 9108, 2994, 136, 506, 0]"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["encoded_tokens = tokenizer.encode(input_sentence)\n","encoded_tokens"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":2575,"status":"ok","timestamp":1699399281045,"user":{"displayName":"Chris Alexiuk","userId":"06521420203810986327"},"user_tz":300},"id":"oS6lE-NLRnzk","outputId":"c67bd98b-11dc-4362-f60e-c15ebdd59d7e"},"outputs":[{"data":{"text/plain":["\"Hark, my name be Romeo! I am but a beautiful summer's day!\""]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["decoded_tokens = tokenizer.decode(encoded_tokens)\n","decoded_tokens"]},{"cell_type":"markdown","metadata":{"id":"ji3sF-rA21YH"},"source":["## Tokenizing Dataset\n","\n","Now that we have trained our tokenizer - let's create a dataset we can leverage with the `nanoGPT` library.\n","\n","We'll simply encode our training and validation data - and then save them in binary files for later!\n","\n","> NOTE: Pay attention to the format you want your dataset in. We want ids, which means we want to use the [`.encode()`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizers.Tokenizer.encode) method of our tokenizer."]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1941,"status":"ok","timestamp":1699399282984,"user":{"displayName":"Chris Alexiuk","userId":"06521420203810986327"},"user_tz":300},"id":"calHML6JPnCU","outputId":"e1e381e5-cd31-46d6-e155-5950f338bf56"},"outputs":[{"name":"stdout","output_type":"stream","text":["train has 291,285 tokens\n","val has 34,222 tokens\n"]}],"source":["train_ids = tokenizer.encode(train_data)\n","val_ids = tokenizer.encode(val_data)\n","print(f\"train has {len(train_ids):,} tokens\")\n","print(f\"val has {len(val_ids):,} tokens\")"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"nKJ1KqiiPkRh"},"outputs":[],"source":["# export to bin files\n","# data_path = \"/data/shakespeare/\"\n","data_path = base_path / \"data/shakespeare/\"\n","\n","train_ids = np.array(train_ids, dtype=np.uint16)\n","val_ids = np.array(val_ids, dtype=np.uint16)\n","train_ids.tofile(data_path / 'train.bin')\n","val_ids.tofile(data_path / 'val.bin')\n","# train_ids.tofile(os.path.join(os.path.dirname(data_path), 'train.bin'))\n","# val_ids.tofile(os.path.join(os.path.dirname(data_path), 'val.bin'))"]},{"cell_type":"markdown","metadata":{"id":"c0I3VrRC3XIO"},"source":["## Training The Model\n","\n","Now that we have our tokenized dataset, let's get to training our model!\n","\n","We have a lot of set-up to do before we click \"`.train()`\", so let's jump right into it!\n","\n","First, let's literally jump into the `nanoGPT` repository we cloned earlier."]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1699399282984,"user":{"displayName":"Chris Alexiuk","userId":"06521420203810986327"},"user_tz":300},"id":"NUU2jaalUdqm","outputId":"b6329120-f5e0-48b8-9912-7ec201f96db4"},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/paperspace/llm-engineering-course/hw1/nanoGPT\n"]},{"name":"stderr","output_type":"stream","text":["/home/paperspace/miniconda3/envs/llm-eng/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n","  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"]}],"source":["%cd nanoGPT"]},{"cell_type":"markdown","metadata":{"id":"13p1e8sa3k0V"},"source":["We'll do some critical imports."]},{"cell_type":"code","execution_count":22,"metadata":{"id":"weNR37BwUYNg"},"outputs":[],"source":["import os\n","import time\n","import math\n","import pickle\n","from contextlib import nullcontext\n","\n","import numpy as np\n","import torch\n","\n","# from the local repo\n","from model import GPTConfig, GPT"]},{"cell_type":"markdown","metadata":{"id":"kY_vWZG-3uM-"},"source":["### Hyper-Parameters\n","\n","We have a laundry list of hyper-parameters to set up - let's walk through them and what they mean."]},{"cell_type":"markdown","metadata":{"id":"OykCjVQK5EX-"},"source":["#### I/O\n","\n","- `out_dir` - simple enough, this is the output directory where our checkpoints are saved"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"viM3qlWt5PVS"},"outputs":[],"source":["out_dir = base_path / 'out'"]},{"cell_type":"markdown","metadata":{"id":"A5iwwrNL5H4C"},"source":["#### Initialization\n","\n","Since we're training from scratch, we'll use `init_from = 'scratch'`."]},{"cell_type":"code","execution_count":26,"metadata":{"id":"OK1z2m3C312T"},"outputs":[],"source":["init_from = 'scratch'"]},{"cell_type":"markdown","metadata":{"id":"2YlolKOj4_dE"},"source":["#### Eval and Logging\n","\n","- `eval_interval` - this is the number of steps between evaluation stages, we'll want to see this ~`250`. Our model will be incredibly prone to over-fitting, and this will let us monitor with relative frequency.\n","- `log_interval` - this is how often our training progress will log. You can set this ~`10`. It's dealer's choice, really.\n","- `eval_iters` - this is how *many* iterations we want to evaluate for.\n","- `eval_only` - this would evaluate our model - but not train it. We'll leave this as `False` for now.\n","- `always_save_checkpoint` - this will always save our most recent checkpoint, regardless of metrics. For this example, we'll set this to `True`."]},{"cell_type":"code","execution_count":27,"metadata":{"id":"MbFN5Ltq4_mo"},"outputs":[],"source":["eval_interval = 250\n","eval_iters = 10 \n","log_interval = 10 \n","eval_only = False\n","always_save_checkpoint = True"]},{"cell_type":"markdown","metadata":{"id":"a488zaF_4zQk"},"source":["#### Dataset\n","\n","We can set our dataset here - we'll use the one we created earlier!"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"_QC7vWXC40Hp"},"outputs":[],"source":["dataset = 'shakespeare'"]},{"cell_type":"markdown","metadata":{"id":"XP9rBgGc426Q"},"source":["#### Typical Hyper-Parameters\n","\n","- `gradient_accumulation_steps` - we can use gradient accumulation to \"simulate\" larger batch sizes by combining multiple different optimization steps together, without needing the additional memory for large batch sizes. We don't need to worry so much about this for the toy problem - but this hyper-parameter can be configured for larger training runs. [Here](https://lightning.ai/blog/gradient-accumulation/) is some great reading on the topic.\n","- `batch_size` - Typical batch_size - the larger the merrier (up to a point) we'll be using `16` to ensure we do not exceed the memory quota of our GPU.\n","- `block_size` - this can be thought of as another term for the `context window` of our model. Since our model cannot take variable length inputs - we use this to set all inputs to our desired size. We'll use a value of `512` to ensure speedy training."]},{"cell_type":"code","execution_count":29,"metadata":{"id":"EM_ybLPP43Pd"},"outputs":[],"source":["gradient_accumulation_steps = 1\n","batch_size = 128 \n","block_size = 512 "]},{"cell_type":"markdown","metadata":{"id":"UZ-8bDIY45GS"},"source":["#### Model Architecture\n","\n","- `n_layer` - this is the number of decoder layers we will use in our model. More would be considered better (up to a point) and the original GPT-2 paper uses `12`, but we will be using a truncated `6` for ease and speed of training.\n","- `n_head` - this is the number of attention heads in each decoder layer!\n","- `n_embd` - this is the embedding dimension of our model, this is analagous to our `model_d` from the previous notebook. A default value of ~`500` should do the trick!\n","- `dropout` - this sets our dropout value, since our model is small and going to be extremely prone to overfitting, consider setting this at a fairly aggresive level (`0.2` was used in the example training found in the notebook`).\n","- `bias` - wether or not to use bias inside the LayerNorm/Linear layers."]},{"cell_type":"code","execution_count":30,"metadata":{"id":"gMyyDBxB6k4H"},"outputs":[],"source":["n_layer = 12\n","n_head =  12 \n","n_embd = n_head * 64\n","dropout = 0.4\n","bias = False"]},{"cell_type":"markdown","metadata":{"id":"piNHkSaRNDjM"},"source":["#### ❓ QUESTION:\n","\n","What condition must be true as it relates to the `n_embd` and `n_head`?\n","\n","`n_embd % n_head == 0`; not exactly sure why."]},{"cell_type":"markdown","metadata":{"id":"3NWDTaAz7gwh"},"source":["#### Optimizer Hyper-Parameters\n","\n","Basic Optimizer Hyper-Parameters:\n","\n","- `learning_rate` - it's our learning rate! We'll want to set this fairly high ~`1e-3` since we're training on such a small dataset.\n","- `max_iters` - how many iterations do we train for. More iters means longer training times. Feel free to tinker with this value! `5000` is a great place to start.\n","\n","Learning Rate Decay Settings:\n","\n","- `decay_lr` - set decay flag\n","- `weight_Decay` - how much to decay lr by\n","- `lr_decay_iters` - should be set to ~max_iters.\n","- `min_lr` - the minimum lr, should be ~ lr / 10\n","\n","Clipping and Warmup:\n","\n","- `grad_clip` - value to clip gradients to. useful for preventing vanishing gradients.\n","- `warmup_iters` - how many iterations to warmup for. Warmup is useful to allow your training to slowly warmup. It will use a low lr for a number of steps to avoid any massive initial spikes. Since we're training a very small model - we can avoid using many wamrup steps.\n","\n","> NOTE: Many learnings taken from the [Chincilla paper](https://arxiv.org/pdf/2203.15556.pdf) for selecting default or appropriate values."]},{"cell_type":"code","execution_count":31,"metadata":{"id":"qe-669jwUptI"},"outputs":[],"source":["# adamw optimizer\n","learning_rate = 1e-3\n","max_iters = 5000\n","beta1 = 0.9\n","beta2 = 0.99\n","\n","# lr decay settings\n","decay_lr = True\n","weight_decay = 1.0\n","lr_decay_iters = max_iters\n","min_lr = learning_rate/10\n","\n","# clipping and warmup\n","grad_clip = 1.0\n","warmup_iters = 100"]},{"cell_type":"markdown","metadata":{"id":"AzHvpMDTNfU6"},"source":["#### ❓ QUESTION:\n","\n","Given a Learning Rate of `1e-4` and a maximum iteration cap of `10,000`: What should `lr_decay_iters` be, and what should `min_lr` be?\n","\n","`min_lr = 1e-4/10`\n","\n","`lr_decay_iters = 10000`"]},{"cell_type":"markdown","metadata":{"id":"ucldc4mz9yeT"},"source":["These hyper-parameters are necessary to set given the task we're training and given the environment we're training in."]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1699399283149,"user":{"displayName":"Chris Alexiuk","userId":"06521420203810986327"},"user_tz":300},"id":"xHiGlMOp8Nux","outputId":"381c2691-5dc5-409a-ecfe-e58ba9ede83b"},"outputs":[{"name":"stdout","output_type":"stream","text":["tokens per iteration will be: 65,536\n"]}],"source":["backend = 'nccl'\n","device = 'cuda'\n","dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n","compile = True\n","# -----------------------------------------------------------------------------\n","config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n","config = {k: globals()[k] for k in config_keys}\n","# -----------------------------------------------------------------------------\n","master_process = True\n","seed_offset = 0\n","ddp_world_size = 1\n","tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n","print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n","os.makedirs(out_dir, exist_ok=True)"]},{"cell_type":"markdown","metadata":{"id":"eKmdfbye-BNf"},"source":["### Torch Settings\n","\n","We need to set a few `torch` settings, including the seed, to allow us to train correctly on our GPU.\n","\n","Not much is required for us to understand here - these are just necessary lines of code. Boilerplate."]},{"cell_type":"code","execution_count":44,"metadata":{"id":"yh34QGD6VARU"},"outputs":[],"source":["torch.manual_seed(1337 + seed_offset)\n","torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n","torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n","device_type = 'cuda' if 'cuda' in device else 'cpu'\n","ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n","ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"]},{"cell_type":"markdown","metadata":{"id":"gKeNwYaZ-Zoc"},"source":["### Dataloader\n","\n","This block will:\n","\n","1. Set the data path\n","2. Load the dataset we tokenized earlier from the `.bin` we saved\n","3. Define a `get_batch` function that will return us a random section of our data as well as a the corresponding \"label\" for that data and move it to the GPU for easy use inside our training loop."]},{"cell_type":"code","execution_count":45,"metadata":{"id":"tOjaPyJpVEgx"},"outputs":[],"source":["data_dir = os.path.join('/data', dataset)\n","data_dir = base_path / \"data\" / dataset\n","train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n","val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n","\n","def get_batch(split):\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n","    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n","    if device_type == 'cuda':\n","        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n","        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n","    else:\n","        x, y = x.to(device), y.to(device)\n","    return x, y"]},{"cell_type":"markdown","metadata":{"id":"I-tifZVD-9hG"},"source":["#### ❓ Question:\n","\n","What can you tell us about the way the labels are generated?\n","1.  First a random set of size `batch_size` of indices from the interval `[a, b)` is chosen\n","where `a = 1` and `b = len(data) - block_size`.  We subtract `block_size` so that we\n","do not \"fall off the end\" of the data.\n","2.  To get a data point, `x`, choose an index, `i`, from the set above; then get a `block_size` of\n","tokens from data starting at position `i` (`data[i:i+block_size]`).\n","3.  To get a label correspond to the data point from 2., shift right one token (`data[i+1:i+1+block_size]`).\n","\n","I think this is done so that the model is pre-trained by \"trying\" to predict the next token given\n","previous token.\n","\n","Please produce an example of a single x and y pair."]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["x: [2087    4  491  131  428]\n","y: [  4 491 131 428   6]\n"]}],"source":["i = 10\n","x = train_data[i:i+block_size].astype(np.int64)\n","y = train_data[i+1:i+1+block_size].astype(np.int64)\n","print(f\"x: {x[:5]}\")\n","print(f\"y: {y[:5]}\")"]},{"cell_type":"markdown","metadata":{"id":"EbDlW-68_atH"},"source":["### Simple Initialization of Model\n","\n","Here we init our number of iterations as 0, and our best val loss as a very high number."]},{"cell_type":"code","execution_count":47,"metadata":{"id":"6hsepdVBVzQU"},"outputs":[],"source":["iter_num = 0\n","best_val_loss = 1e9"]},{"cell_type":"markdown","metadata":{"id":"A4Uj9qBI_vXc"},"source":["Obtain our vocab size from our trained tokenizer."]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1699399283150,"user":{"displayName":"Chris Alexiuk","userId":"06521420203810986327"},"user_tz":300},"id":"m53DcCdFV0_a","outputId":"0d225488-fa73-4714-ff69-f39c16af4d25"},"outputs":[{"data":{"text/plain":["20094"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["meta_path = os.path.join(data_dir, 'meta.pkl')\n","meta_vocab_size = tokenizer.vocab_size\n","meta_vocab_size"]},{"cell_type":"markdown","metadata":{"id":"V7bcNelYARmD"},"source":["Create our model args dict.\n","\n","Use the following as a guide: [Here](https://github.com/karpathy/nanoGPT/blob/eba36e84649f3c6d840a93092cb779a260544d08/model.py#L109)"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"JfIWEbanV7ZS"},"outputs":[],"source":["model_args = dict( \n","    block_size = block_size,\n","    vocab_size = meta_vocab_size,\n","    n_layer = n_layer,\n","    n_head = n_head,\n","    n_embd = n_embd,\n","    dropout = dropout,\n","    bias = True \n",")"]},{"cell_type":"markdown","metadata":{"id":"2WWcbkiCAUI2"},"source":["Instantiate our model with the provided `model_args`.\n","\n","These are derived from the hyper-parameters we set above."]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":760,"status":"ok","timestamp":1699399283906,"user":{"displayName":"Chris Alexiuk","userId":"06521420203810986327"},"user_tz":300},"id":"1Xly4iA0V-vF","outputId":"9b192cd6-967d-4c4f-df14-9651b6c5bac9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Initializing a new model from scratch\n","number of parameters: 100.49M\n"]}],"source":["if init_from == 'scratch':\n","    print(\"Initializing a new model from scratch\")\n","    if meta_vocab_size is None:\n","        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n","    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n","    gptconf = GPTConfig(**model_args)\n","    model = GPT(gptconf)"]},{"cell_type":"markdown","metadata":{"id":"BpViOsxLAl6p"},"source":["There we go! If you used the default values - you should have a model with 29.55M parameters!\n","\n","Let's set our block_size to the correct size as determined in our configuration steps."]},{"cell_type":"code","execution_count":51,"metadata":{"id":"TrEawNxdWRhm"},"outputs":[],"source":["if block_size < model.config.block_size:\n","    model.crop_block_size(block_size)\n","    model_args['block_size'] = block_size"]},{"cell_type":"markdown","metadata":{"id":"eRgguPLKAuZ5"},"source":["Now we can look at our model in all its glory!"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5659,"status":"ok","timestamp":1699399289563,"user":{"displayName":"Chris Alexiuk","userId":"06521420203810986327"},"user_tz":300},"id":"zaE3KSTnAtJs","outputId":"9dcae01f-2bdd-4d63-cb1e-8801dcaae293"},"outputs":[{"data":{"text/plain":["GPT(\n","  (transformer): ModuleDict(\n","    (wte): Embedding(20094, 768)\n","    (wpe): Embedding(512, 768)\n","    (drop): Dropout(p=0.4, inplace=False)\n","    (h): ModuleList(\n","      (0-11): 12 x Block(\n","        (ln_1): LayerNorm()\n","        (attn): CausalSelfAttention(\n","          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n","          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n","          (attn_dropout): Dropout(p=0.4, inplace=False)\n","          (resid_dropout): Dropout(p=0.4, inplace=False)\n","        )\n","        (ln_2): LayerNorm()\n","        (mlp): MLP(\n","          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","          (gelu): GELU(approximate='none')\n","          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.4, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm()\n","  )\n","  (lm_head): Linear(in_features=768, out_features=20094, bias=False)\n",")"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"LzoEY6gcBOSp"},"source":["We'll set up our GradScaler - more information on this process [here](https://pytorch.org/docs/stable/amp.html#gradient-scaling)."]},{"cell_type":"code","execution_count":53,"metadata":{"id":"BNUThRt4WT5H"},"outputs":[],"source":["scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"]},{"cell_type":"markdown","metadata":{"id":"6Zs5Hcf9BBUD"},"source":["Let's set up our optimizer below. Be sure to include the correct values. You can check the `model.py` file for more information on what is expected in the `configure_optimizers` method [here](https://github.com/karpathy/nanoGPT/blob/eba36e84649f3c6d840a93092cb779a260544d08/model.py#L263C85-L263C85)."]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":413,"status":"ok","timestamp":1699399289975,"user":{"displayName":"Chris Alexiuk","userId":"06521420203810986327"},"user_tz":300},"id":"YesGeUnoWViL","outputId":"34e3f54b-22e0-4809-a435-f41423e3d1ae"},"outputs":[{"name":"stdout","output_type":"stream","text":["num decayed parameter tensors: 50, with 100,760,064 parameters\n","num non-decayed parameter tensors: 98, with 121,344 parameters\n","using fused AdamW: True\n"]}],"source":["optimizer = model.configure_optimizers(\n","    weight_decay,\n","    learning_rate,\n","    (beta1, beta2),\n","    device_type\n",")\n","\n","checkpoint = None"]},{"cell_type":"markdown","metadata":{"id":"ZF5YWJoKB4og"},"source":["Now we can compile our model!\n","\n","If you're using the T4 or V100 instance of Colab - this will not provide a signficant speed-up, but if you're using Ampere architecture (A100) you should notice a significant difference between the compiled and uncompiled model.\n","\n","Read more about `torch.compile()` [here](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)."]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2626,"status":"ok","timestamp":1699399292597,"user":{"displayName":"Chris Alexiuk","userId":"06521420203810986327"},"user_tz":300},"id":"v0FNU0T0WXdI","outputId":"c4181a53-d43b-41c0-fd84-4c514673fade"},"outputs":[{"name":"stdout","output_type":"stream","text":["compiling the model... (takes a ~minute)\n"]},{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]}],"source":["if compile:\n","    print(\"compiling the model... (takes a ~minute)\")\n","    unoptimized_model = model\n","    model = torch.compile(model) # requires PyTorch 2.0"]},{"cell_type":"markdown","metadata":{"id":"p6lRcVsZCXRO"},"source":["We'll set up our loss estimation function here, which will help us estimate an arbitrarily accurate loss over either training or validation data by using many batches.\n","\n","You'll notice that we quickly convert the model into `.eval()` model and then back to `.train()` mode."]},{"cell_type":"code","execution_count":56,"metadata":{"id":"lUB5zVLVWbhM"},"outputs":[],"source":["@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)\n","            with ctx:\n","                logits, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out"]},{"cell_type":"markdown","metadata":{"id":"fLsOpaACDDkF"},"source":["### Creating our LR Scheduler\n","\n","Beyond just slowly reducing our learning rate over time - we can use an LR Scheduler to allow us to move our learning according to a desired pattern.\n","\n","We will use a \"cosine with warmup\" schedule and our learning rate, thusly, will follow this pattern:\n","\n","![img](https://i.imgur.com/KoFEl0b.png)\n","\n","There are many different schedulers, and many different ways to handle learning rate, and you can read about just a few of them [here](https://d2l.ai/chapter_optimization/lr-scheduler.html)!"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"7-mNpWBSWdHh"},"outputs":[],"source":["def get_lr(it):\n","    # 1) linear warmup for warmup_iters steps\n","    if it < warmup_iters:\n","        return learning_rate * it / warmup_iters\n","    # 2) if it > lr_decay_iters, return min learning rate\n","    if it > lr_decay_iters:\n","        return min_lr\n","    # 3) in between, use cosine decay down to min learning rate\n","    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n","    assert 0 <= decay_ratio <= 1\n","    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n","    return min_lr + coeff * (learning_rate - min_lr)"]},{"cell_type":"markdown","metadata":{"id":"cqFePCZmE1Lq"},"source":["We need to set some specific values in our env to allow training in Colab."]},{"cell_type":"code","execution_count":58,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2841,"status":"ok","timestamp":1699399295437,"user":{"displayName":"Chris Alexiuk","userId":"06521420203810986327"},"user_tz":300},"id":"-7nDL6s4YT6E","outputId":"c619d8db-8d4e-491d-b518-82f86bdbb8df"},"outputs":[],"source":["# !export LC_ALL=\"en_US.UTF-8\"\n","# !export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n","# !export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n","# !ldconfig /usr/lib64-nvidia"]},{"cell_type":"markdown","metadata":{"id":"Nhqmxeo0Eg0Z"},"source":["## The Training Loop\n","\n","Now we can finally grab our first batch and set our initial time to calculate how long our iterations are taking!"]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1517805,"status":"ok","timestamp":1699400813240,"user":{"displayName":"Chris Alexiuk","userId":"06521420203810986327"},"user_tz":300},"id":"kHbyEapRWmpc","outputId":"b93a0c7c-97d8-4fe9-ed71-875249952fc7"},"outputs":[{"name":"stdout","output_type":"stream","text":["step 0: train loss 10.0417, val loss 10.0452\n","iter 0: loss 10.0393, time 37416.80ms, mfu -100.00%\n","iter 10: loss 7.9366, time 321.43ms, mfu 43.10%\n","iter 20: loss 6.6990, time 321.98ms, mfu 43.09%\n","iter 30: loss 5.9097, time 321.80ms, mfu 43.09%\n","iter 40: loss 5.6520, time 322.53ms, mfu 43.08%\n","iter 50: loss 5.4340, time 322.93ms, mfu 43.06%\n","iter 60: loss 5.2163, time 323.04ms, mfu 43.04%\n","iter 70: loss 5.0008, time 323.61ms, mfu 43.02%\n","iter 80: loss 4.8323, time 323.53ms, mfu 43.00%\n","iter 90: loss 4.6810, time 323.91ms, mfu 42.98%\n","iter 100: loss 4.5772, time 325.00ms, mfu 42.94%\n","iter 110: loss 4.4567, time 324.04ms, mfu 42.92%\n","iter 120: loss 4.3930, time 324.32ms, mfu 42.90%\n","iter 130: loss 4.1994, time 325.17ms, mfu 42.87%\n","iter 140: loss 4.1690, time 325.06ms, mfu 42.85%\n","iter 150: loss 4.0846, time 324.30ms, mfu 42.83%\n","iter 160: loss 3.9569, time 323.92ms, mfu 42.83%\n","iter 170: loss 3.9564, time 324.75ms, mfu 42.81%\n","iter 180: loss 3.8598, time 324.10ms, mfu 42.80%\n","iter 190: loss 3.7840, time 324.70ms, mfu 42.79%\n","iter 200: loss 3.6980, time 324.68ms, mfu 42.78%\n","iter 210: loss 3.6598, time 324.79ms, mfu 42.77%\n","iter 220: loss 3.5637, time 325.58ms, mfu 42.74%\n","iter 230: loss 3.5211, time 325.05ms, mfu 42.73%\n","iter 240: loss 3.4748, time 324.58ms, mfu 42.73%\n","step 250: train loss 3.2874, val loss 5.2113\n","saving checkpoint to /home/paperspace/llm-engineering-course/hw1/out\n","iter 250: loss 3.3859, time 6584.10ms, mfu 38.67%\n","iter 260: loss 3.3172, time 324.20ms, mfu 39.07%\n","iter 270: loss 3.2920, time 324.32ms, mfu 39.44%\n","iter 280: loss 3.1890, time 324.77ms, mfu 39.76%\n","iter 290: loss 3.1273, time 325.60ms, mfu 40.04%\n","iter 300: loss 3.0720, time 324.47ms, mfu 40.30%\n","iter 310: loss 2.9493, time 324.85ms, mfu 40.54%\n","iter 320: loss 2.8966, time 325.26ms, mfu 40.74%\n","iter 330: loss 2.8175, time 324.82ms, mfu 40.93%\n","iter 340: loss 2.7209, time 325.12ms, mfu 41.10%\n","iter 350: loss 2.6439, time 324.86ms, mfu 41.26%\n","iter 360: loss 2.5961, time 324.56ms, mfu 41.40%\n","iter 370: loss 2.4600, time 325.77ms, mfu 41.51%\n","iter 380: loss 2.4086, time 325.21ms, mfu 41.62%\n","iter 390: loss 2.2506, time 324.77ms, mfu 41.72%\n","iter 400: loss 2.1534, time 324.78ms, mfu 41.82%\n","iter 410: loss 2.0874, time 324.68ms, mfu 41.90%\n","iter 420: loss 1.9540, time 325.70ms, mfu 41.97%\n","iter 430: loss 1.8372, time 324.78ms, mfu 42.04%\n","iter 440: loss 1.7545, time 325.52ms, mfu 42.09%\n","iter 450: loss 1.6423, time 324.96ms, mfu 42.14%\n","iter 460: loss 1.5610, time 325.29ms, mfu 42.19%\n","iter 470: loss 1.4174, time 324.97ms, mfu 42.23%\n","iter 480: loss 1.3590, time 325.48ms, mfu 42.26%\n","iter 490: loss 1.2477, time 325.50ms, mfu 42.29%\n","step 500: train loss 0.8551, val loss 6.4198\n","saving checkpoint to /home/paperspace/llm-engineering-course/hw1/out\n","iter 500: loss 1.1836, time 6624.31ms, mfu 38.27%\n","iter 510: loss 1.0911, time 324.73ms, mfu 38.71%\n","iter 520: loss 1.0348, time 324.51ms, mfu 39.11%\n","iter 530: loss 0.9804, time 325.05ms, mfu 39.46%\n","iter 540: loss 0.8898, time 327.02ms, mfu 39.75%\n","iter 550: loss 0.8450, time 325.78ms, mfu 40.03%\n","iter 560: loss 0.8372, time 325.30ms, mfu 40.29%\n","iter 570: loss 0.7303, time 325.20ms, mfu 40.52%\n","iter 580: loss 0.7130, time 325.07ms, mfu 40.73%\n","iter 590: loss 0.6762, time 324.93ms, mfu 40.92%\n","iter 600: loss 0.6250, time 324.96ms, mfu 41.09%\n","iter 610: loss 0.6075, time 325.51ms, mfu 41.24%\n","iter 620: loss 0.5559, time 324.84ms, mfu 41.38%\n","iter 630: loss 0.5335, time 325.00ms, mfu 41.50%\n","iter 640: loss 0.4952, time 325.29ms, mfu 41.61%\n","iter 650: loss 0.4787, time 325.37ms, mfu 41.71%\n","iter 660: loss 0.4680, time 324.69ms, mfu 41.80%\n","iter 670: loss 0.4402, time 325.28ms, mfu 41.88%\n","iter 680: loss 0.4263, time 324.95ms, mfu 41.96%\n","iter 690: loss 0.4034, time 325.60ms, mfu 42.02%\n","iter 700: loss 0.3912, time 324.45ms, mfu 42.09%\n","iter 710: loss 0.3686, time 325.06ms, mfu 42.14%\n","iter 720: loss 0.3583, time 325.33ms, mfu 42.18%\n","iter 730: loss 0.3845, time 324.84ms, mfu 42.23%\n","iter 740: loss 0.3423, time 325.12ms, mfu 42.27%\n","step 750: train loss 0.1452, val loss 7.6776\n","saving checkpoint to /home/paperspace/llm-engineering-course/hw1/out\n","iter 750: loss 0.3336, time 6573.34ms, mfu 38.25%\n","iter 760: loss 0.3087, time 324.35ms, mfu 38.70%\n","iter 770: loss 0.2995, time 323.78ms, mfu 39.11%\n","iter 780: loss 0.3043, time 324.30ms, mfu 39.47%\n","iter 790: loss 0.2931, time 324.76ms, mfu 39.79%\n","iter 800: loss 0.2767, time 324.63ms, mfu 40.08%\n","iter 810: loss 0.2676, time 324.88ms, mfu 40.33%\n","iter 820: loss 0.2698, time 325.37ms, mfu 40.56%\n","iter 830: loss 0.2704, time 324.92ms, mfu 40.77%\n","iter 840: loss 0.2644, time 325.69ms, mfu 40.94%\n","iter 850: loss 0.2646, time 324.52ms, mfu 41.12%\n","iter 860: loss 0.2731, time 325.09ms, mfu 41.27%\n","iter 870: loss 0.2455, time 324.92ms, mfu 41.40%\n","iter 880: loss 0.2459, time 325.67ms, mfu 41.52%\n","iter 890: loss 0.2515, time 324.73ms, mfu 41.63%\n","iter 900: loss 0.2442, time 324.76ms, mfu 41.74%\n","iter 910: loss 0.2344, time 324.82ms, mfu 41.83%\n","iter 920: loss 0.2290, time 324.78ms, mfu 41.91%\n","iter 930: loss 0.2294, time 325.17ms, mfu 41.98%\n","iter 940: loss 0.2262, time 325.25ms, mfu 42.04%\n","iter 950: loss 0.2178, time 325.09ms, mfu 42.10%\n","iter 960: loss 0.2341, time 325.53ms, mfu 42.14%\n","iter 970: loss 0.2091, time 324.72ms, mfu 42.20%\n","iter 980: loss 0.2169, time 325.21ms, mfu 42.24%\n","iter 990: loss 0.2031, time 325.22ms, mfu 42.27%\n","step 1000: train loss 0.0842, val loss 8.2140\n","saving checkpoint to /home/paperspace/llm-engineering-course/hw1/out\n","iter 1000: loss 0.2094, time 6747.44ms, mfu 38.25%\n","iter 1010: loss 0.2033, time 324.43ms, mfu 38.70%\n","iter 1020: loss 0.2039, time 324.55ms, mfu 39.10%\n","iter 1030: loss 0.2029, time 324.82ms, mfu 39.45%\n","iter 1040: loss 0.1977, time 324.96ms, mfu 39.77%\n","iter 1050: loss 0.2041, time 324.37ms, mfu 40.06%\n","iter 1060: loss 0.1932, time 324.72ms, mfu 40.32%\n","iter 1070: loss 0.1931, time 325.37ms, mfu 40.55%\n","iter 1080: loss 0.1886, time 324.99ms, mfu 40.76%\n","iter 1090: loss 0.1940, time 324.35ms, mfu 40.95%\n","iter 1100: loss 0.1812, time 324.74ms, mfu 41.12%\n","iter 1110: loss 0.1909, time 324.94ms, mfu 41.27%\n","iter 1120: loss 0.1842, time 324.48ms, mfu 41.42%\n","iter 1130: loss 0.1789, time 325.46ms, mfu 41.53%\n","iter 1140: loss 0.1838, time 325.43ms, mfu 41.64%\n","iter 1150: loss 0.1715, time 324.82ms, mfu 41.74%\n","iter 1160: loss 0.1853, time 325.02ms, mfu 41.83%\n","iter 1170: loss 0.2848, time 324.90ms, mfu 41.91%\n","iter 1180: loss 0.2324, time 325.06ms, mfu 41.98%\n","iter 1190: loss 0.2000, time 325.79ms, mfu 42.03%\n","iter 1200: loss 0.1883, time 325.53ms, mfu 42.09%\n","iter 1210: loss 0.1669, time 324.93ms, mfu 42.14%\n","iter 1220: loss 0.1506, time 324.94ms, mfu 42.19%\n","iter 1230: loss 0.1529, time 324.57ms, mfu 42.24%\n","iter 1240: loss 0.1421, time 325.81ms, mfu 42.27%\n","step 1250: train loss 0.0627, val loss 8.4365\n","saving checkpoint to /home/paperspace/llm-engineering-course/hw1/out\n","iter 1250: loss 0.1460, time 6703.28ms, mfu 38.25%\n","iter 1260: loss 0.1586, time 323.90ms, mfu 38.70%\n","iter 1270: loss 0.2347, time 325.14ms, mfu 39.09%\n","iter 1280: loss 0.1927, time 324.66ms, mfu 39.45%\n","iter 1290: loss 0.1694, time 325.00ms, mfu 39.77%\n","iter 1300: loss 0.1605, time 324.43ms, mfu 40.06%\n","iter 1310: loss 0.1509, time 325.01ms, mfu 40.32%\n","iter 1320: loss 0.1449, time 325.25ms, mfu 40.54%\n","iter 1330: loss 0.1425, time 325.31ms, mfu 40.75%\n","iter 1340: loss 0.1368, time 324.91ms, mfu 40.94%\n","iter 1350: loss 0.1390, time 325.05ms, mfu 41.11%\n","iter 1360: loss 0.1385, time 325.61ms, mfu 41.25%\n","iter 1370: loss 0.1357, time 325.22ms, mfu 41.39%\n","iter 1380: loss 0.1393, time 325.50ms, mfu 41.50%\n","iter 1390: loss 0.1435, time 325.25ms, mfu 41.61%\n","iter 1400: loss 0.1416, time 325.40ms, mfu 41.71%\n","iter 1410: loss 0.1409, time 325.41ms, mfu 41.80%\n","iter 1420: loss 0.1390, time 325.49ms, mfu 41.87%\n","iter 1430: loss 0.1463, time 324.67ms, mfu 41.95%\n","iter 1440: loss 0.1360, time 324.68ms, mfu 42.02%\n","iter 1450: loss 0.1411, time 325.12ms, mfu 42.08%\n","iter 1460: loss 0.1335, time 324.84ms, mfu 42.14%\n","iter 1470: loss 0.1386, time 325.33ms, mfu 42.18%\n","iter 1480: loss 0.1556, time 324.84ms, mfu 42.23%\n","iter 1490: loss 0.1498, time 325.06ms, mfu 42.27%\n","step 1500: train loss 0.0568, val loss 8.5563\n","saving checkpoint to /home/paperspace/llm-engineering-course/hw1/out\n","iter 1500: loss 0.1448, time 6509.13ms, mfu 38.25%\n","iter 1510: loss 0.1350, time 323.98ms, mfu 38.71%\n","iter 1520: loss 0.1328, time 324.96ms, mfu 39.10%\n","iter 1530: loss 0.1278, time 324.05ms, mfu 39.46%\n","iter 1540: loss 0.1335, time 324.75ms, mfu 39.78%\n","iter 1550: loss 0.1316, time 324.85ms, mfu 40.07%\n","iter 1560: loss 0.1222, time 325.20ms, mfu 40.32%\n","iter 1570: loss 0.1256, time 325.01ms, mfu 40.55%\n","iter 1580: loss 0.1253, time 324.98ms, mfu 40.76%\n","iter 1590: loss 0.1282, time 324.65ms, mfu 40.95%\n","iter 1600: loss 0.1243, time 325.13ms, mfu 41.12%\n","iter 1610: loss 0.1254, time 325.12ms, mfu 41.27%\n","iter 1620: loss 0.1239, time 324.93ms, mfu 41.40%\n","iter 1630: loss 0.1276, time 324.66ms, mfu 41.53%\n","iter 1640: loss 0.1219, time 324.98ms, mfu 41.64%\n","iter 1650: loss 0.1096, time 324.62ms, mfu 41.74%\n","iter 1660: loss 0.1215, time 325.84ms, mfu 41.82%\n","iter 1670: loss 0.1184, time 324.81ms, mfu 41.91%\n","iter 1680: loss 0.1212, time 325.30ms, mfu 41.97%\n","iter 1690: loss 0.1194, time 325.16ms, mfu 42.04%\n","iter 1700: loss 0.1184, time 324.64ms, mfu 42.10%\n","iter 1710: loss 0.1199, time 325.00ms, mfu 42.15%\n","iter 1720: loss 0.1185, time 325.64ms, mfu 42.19%\n","iter 1730: loss 0.1225, time 325.52ms, mfu 42.23%\n","iter 1740: loss 0.1167, time 325.25ms, mfu 42.27%\n","step 1750: train loss 0.0499, val loss 8.8729\n","saving checkpoint to /home/paperspace/llm-engineering-course/hw1/out\n","iter 1750: loss 0.1160, time 6546.31ms, mfu 38.25%\n","iter 1760: loss 0.1120, time 323.87ms, mfu 38.70%\n","iter 1770: loss 0.1106, time 324.61ms, mfu 39.10%\n","iter 1780: loss 0.1124, time 325.32ms, mfu 39.45%\n","iter 1790: loss 0.1166, time 324.49ms, mfu 39.77%\n","iter 1800: loss 0.1075, time 324.73ms, mfu 40.06%\n","iter 1810: loss 0.1095, time 324.83ms, mfu 40.32%\n","iter 1820: loss 0.1085, time 325.29ms, mfu 40.55%\n","iter 1830: loss 0.1077, time 324.58ms, mfu 40.76%\n","iter 1840: loss 0.1116, time 325.58ms, mfu 40.94%\n","iter 1850: loss 0.1107, time 325.21ms, mfu 41.11%\n","iter 1860: loss 0.1093, time 324.60ms, mfu 41.26%\n","iter 1870: loss 0.1045, time 324.63ms, mfu 41.41%\n","iter 1880: loss 0.1101, time 325.43ms, mfu 41.52%\n","iter 1890: loss 0.1067, time 325.39ms, mfu 41.63%\n","iter 1900: loss 0.1108, time 325.18ms, mfu 41.72%\n","iter 1910: loss 0.1054, time 325.02ms, mfu 41.81%\n","iter 1920: loss 0.1003, time 325.43ms, mfu 41.89%\n","iter 1930: loss 0.1063, time 325.26ms, mfu 41.96%\n","iter 1940: loss 0.1040, time 324.75ms, mfu 42.03%\n","iter 1950: loss 0.0996, time 324.88ms, mfu 42.09%\n","iter 1960: loss 0.1018, time 324.62ms, mfu 42.15%\n","iter 1970: loss 0.1033, time 325.30ms, mfu 42.19%\n","iter 1980: loss 0.1033, time 324.70ms, mfu 42.24%\n","iter 1990: loss 0.1015, time 324.95ms, mfu 42.28%\n","step 2000: train loss 0.0448, val loss 8.9085\n","saving checkpoint to /home/paperspace/llm-engineering-course/hw1/out\n","iter 2000: loss 0.0993, time 6651.22ms, mfu 38.26%\n","iter 2010: loss 0.0960, time 324.58ms, mfu 38.70%\n","iter 2020: loss 0.1031, time 324.91ms, mfu 39.10%\n","iter 2030: loss 0.0979, time 324.33ms, mfu 39.46%\n","iter 2040: loss 0.0964, time 324.67ms, mfu 39.78%\n","iter 2050: loss 0.0974, time 324.38ms, mfu 40.07%\n","iter 2060: loss 0.0976, time 324.31ms, mfu 40.34%\n","iter 2070: loss 0.0958, time 324.37ms, mfu 40.57%\n","iter 2080: loss 0.0937, time 324.65ms, mfu 40.78%\n","iter 2090: loss 0.0917, time 325.21ms, mfu 40.97%\n","iter 2100: loss 0.0916, time 325.24ms, mfu 41.13%\n","iter 2110: loss 0.0973, time 325.61ms, mfu 41.27%\n","iter 2120: loss 0.0989, time 324.92ms, mfu 41.41%\n","iter 2130: loss 0.0962, time 325.45ms, mfu 41.52%\n","iter 2140: loss 0.0991, time 325.67ms, mfu 41.63%\n","iter 2150: loss 0.0939, time 325.03ms, mfu 41.73%\n","iter 2160: loss 0.0900, time 324.56ms, mfu 41.82%\n","iter 2170: loss 0.0905, time 325.17ms, mfu 41.90%\n","iter 2180: loss 0.0917, time 324.81ms, mfu 41.98%\n","iter 2190: loss 0.0931, time 324.90ms, mfu 42.04%\n","iter 2200: loss 0.0912, time 325.08ms, mfu 42.10%\n","iter 2210: loss 0.0925, time 324.97ms, mfu 42.15%\n","iter 2220: loss 0.0881, time 324.93ms, mfu 42.20%\n","iter 2230: loss 0.0898, time 325.86ms, mfu 42.23%\n","iter 2240: loss 0.0874, time 325.98ms, mfu 42.26%\n","step 2250: train loss 0.0402, val loss 9.0396\n","saving checkpoint to /home/paperspace/llm-engineering-course/hw1/out\n","iter 2250: loss 0.0843, time 6554.87ms, mfu 38.24%\n","iter 2260: loss 0.0870, time 325.00ms, mfu 38.68%\n","iter 2270: loss 0.0860, time 324.28ms, mfu 39.09%\n","iter 2280: loss 0.0908, time 324.86ms, mfu 39.44%\n","iter 2290: loss 0.0867, time 325.11ms, mfu 39.76%\n","iter 2300: loss 0.0864, time 324.54ms, mfu 40.05%\n","iter 2310: loss 0.0804, time 324.68ms, mfu 40.31%\n","iter 2320: loss 0.0837, time 324.91ms, mfu 40.55%\n","iter 2330: loss 0.0832, time 325.12ms, mfu 40.75%\n","iter 2340: loss 0.0886, time 325.35ms, mfu 40.94%\n","iter 2350: loss 0.0825, time 325.57ms, mfu 41.10%\n","iter 2360: loss 0.0791, time 325.36ms, mfu 41.25%\n","iter 2370: loss 0.0868, time 324.82ms, mfu 41.39%\n","iter 2380: loss 0.0811, time 325.41ms, mfu 41.51%\n","iter 2390: loss 0.0811, time 325.12ms, mfu 41.62%\n","iter 2400: loss 0.0825, time 325.06ms, mfu 41.72%\n","iter 2410: loss 0.0800, time 325.56ms, mfu 41.80%\n","iter 2420: loss 0.0792, time 325.64ms, mfu 41.87%\n","iter 2430: loss 0.0786, time 325.29ms, mfu 41.95%\n","iter 2440: loss 0.0813, time 325.69ms, mfu 42.01%\n","iter 2450: loss 0.0818, time 325.45ms, mfu 42.06%\n","iter 2460: loss 0.0798, time 325.10ms, mfu 42.12%\n","iter 2470: loss 0.0767, time 325.56ms, mfu 42.16%\n","iter 2480: loss 0.0765, time 325.60ms, mfu 42.20%\n","iter 2490: loss 0.0786, time 325.17ms, mfu 42.24%\n","step 2500: train loss 0.0375, val loss 8.9524\n","saving checkpoint to /home/paperspace/llm-engineering-course/hw1/out\n","iter 2500: loss 0.0785, time 6474.82ms, mfu 38.23%\n","iter 2510: loss 0.0783, time 324.43ms, mfu 38.68%\n","iter 2520: loss 0.0762, time 325.10ms, mfu 39.07%\n","iter 2530: loss 0.0773, time 324.75ms, mfu 39.43%\n","iter 2540: loss 0.0812, time 324.94ms, mfu 39.75%\n","iter 2550: loss 0.0818, time 325.26ms, mfu 40.03%\n","iter 2560: loss 0.0764, time 324.52ms, mfu 40.30%\n","iter 2570: loss 0.0753, time 325.00ms, mfu 40.53%\n","iter 2580: loss 0.0738, time 324.95ms, mfu 40.74%\n","iter 2590: loss 0.0737, time 325.24ms, mfu 40.93%\n","iter 2600: loss 0.0752, time 325.07ms, mfu 41.10%\n","iter 2610: loss 0.0736, time 325.15ms, mfu 41.25%\n","iter 2620: loss 0.0752, time 326.13ms, mfu 41.37%\n","iter 2630: loss 0.0775, time 326.21ms, mfu 41.48%\n","iter 2640: loss 0.0719, time 325.42ms, mfu 41.59%\n","iter 2650: loss 0.0762, time 324.52ms, mfu 41.70%\n","iter 2660: loss 0.0702, time 325.13ms, mfu 41.79%\n","iter 2670: loss 0.0734, time 325.27ms, mfu 41.87%\n","iter 2680: loss 0.0756, time 325.18ms, mfu 41.95%\n","iter 2690: loss 0.0730, time 325.24ms, mfu 42.01%\n","iter 2700: loss 0.0735, time 325.59ms, mfu 42.06%\n","iter 2710: loss 0.0742, time 325.55ms, mfu 42.11%\n","iter 2720: loss 0.0710, time 325.61ms, mfu 42.16%\n","iter 2730: loss 0.0721, time 325.43ms, mfu 42.20%\n","iter 2740: loss 0.0720, time 325.81ms, mfu 42.23%\n","step 2750: train loss 0.0349, val loss 9.0465\n","saving checkpoint to /home/paperspace/llm-engineering-course/hw1/out\n","iter 2750: loss 0.0698, time 6601.12ms, mfu 38.22%\n","iter 2760: loss 0.0697, time 324.24ms, mfu 38.67%\n","iter 2770: loss 0.0671, time 325.01ms, mfu 39.06%\n","iter 2780: loss 0.0646, time 324.81ms, mfu 39.42%\n","iter 2790: loss 0.0647, time 324.66ms, mfu 39.75%\n","iter 2800: loss 0.0671, time 324.87ms, mfu 40.04%\n","iter 2810: loss 0.0654, time 324.84ms, mfu 40.30%\n","iter 2820: loss 0.0651, time 326.38ms, mfu 40.51%\n","iter 2830: loss 0.0670, time 325.48ms, mfu 40.72%\n","iter 2840: loss 0.0609, time 324.75ms, mfu 40.91%\n","iter 2850: loss 0.0645, time 325.64ms, mfu 41.08%\n","iter 2860: loss 0.0637, time 324.70ms, mfu 41.24%\n","iter 2870: loss 0.0621, time 325.25ms, mfu 41.37%\n","iter 2880: loss 0.0630, time 324.98ms, mfu 41.50%\n","iter 2890: loss 0.0662, time 324.69ms, mfu 41.61%\n","iter 2900: loss 0.0642, time 325.38ms, mfu 41.71%\n","iter 2910: loss 0.0630, time 325.40ms, mfu 41.80%\n","iter 2920: loss 0.0652, time 324.99ms, mfu 41.88%\n","iter 2930: loss 0.0626, time 325.57ms, mfu 41.95%\n","iter 2940: loss 0.0618, time 324.66ms, mfu 42.02%\n","iter 2950: loss 0.0638, time 324.97ms, mfu 42.08%\n","iter 2960: loss 0.0615, time 325.61ms, mfu 42.13%\n","iter 2970: loss 0.0639, time 325.61ms, mfu 42.17%\n","iter 2980: loss 0.0633, time 325.82ms, mfu 42.20%\n","iter 2990: loss 0.0631, time 326.30ms, mfu 42.23%\n","step 3000: train loss 0.0325, val loss 9.0856\n","saving checkpoint to /home/paperspace/llm-engineering-course/hw1/out\n","iter 3000: loss 0.0606, time 6430.32ms, mfu 38.22%\n","iter 3010: loss 0.0606, time 324.59ms, mfu 38.67%\n","iter 3020: loss 0.0609, time 324.97ms, mfu 39.06%\n","iter 3030: loss 0.0615, time 325.46ms, mfu 39.42%\n","iter 3040: loss 0.0572, time 325.28ms, mfu 39.73%\n","iter 3050: loss 0.0616, time 325.49ms, mfu 40.02%\n","iter 3060: loss 0.0602, time 326.08ms, mfu 40.26%\n","iter 3070: loss 0.0591, time 324.96ms, mfu 40.50%\n","iter 3080: loss 0.0613, time 325.59ms, mfu 40.70%\n","iter 3090: loss 0.0573, time 326.01ms, mfu 40.88%\n","iter 3100: loss 0.0580, time 325.29ms, mfu 41.05%\n","iter 3110: loss 0.0583, time 325.40ms, mfu 41.21%\n","iter 3120: loss 0.0546, time 325.55ms, mfu 41.34%\n","iter 3130: loss 0.0560, time 325.48ms, mfu 41.46%\n","iter 3140: loss 0.0549, time 325.34ms, mfu 41.58%\n","iter 3150: loss 0.0573, time 325.47ms, mfu 41.67%\n","iter 3160: loss 0.0571, time 325.54ms, mfu 41.76%\n","iter 3170: loss 0.0576, time 325.60ms, mfu 41.84%\n","iter 3180: loss 0.0569, time 325.62ms, mfu 41.91%\n","iter 3190: loss 0.0559, time 325.00ms, mfu 41.98%\n","iter 3200: loss 0.0559, time 325.52ms, mfu 42.04%\n","iter 3210: loss 0.0565, time 325.30ms, mfu 42.10%\n","iter 3220: loss 0.0528, time 325.79ms, mfu 42.14%\n","iter 3230: loss 0.0556, time 325.09ms, mfu 42.19%\n","iter 3240: loss 0.0517, time 325.46ms, mfu 42.22%\n","step 3250: train loss 0.0307, val loss 9.0773\n","saving checkpoint to /home/paperspace/llm-engineering-course/hw1/out\n","iter 3250: loss 0.0551, time 6390.18ms, mfu 38.22%\n","iter 3260: loss 0.0536, time 324.53ms, mfu 38.67%\n","iter 3270: loss 0.0569, time 324.96ms, mfu 39.06%\n","iter 3280: loss 0.0521, time 325.43ms, mfu 39.41%\n","iter 3290: loss 0.0564, time 324.73ms, mfu 39.74%\n","iter 3300: loss 0.0518, time 324.70ms, mfu 40.03%\n","iter 3310: loss 0.0536, time 325.45ms, mfu 40.29%\n","iter 3320: loss 0.0539, time 325.13ms, mfu 40.52%\n","iter 3330: loss 0.0575, time 325.61ms, mfu 40.72%\n","iter 3340: loss 0.0522, time 325.12ms, mfu 40.91%\n","iter 3350: loss 0.0539, time 325.54ms, mfu 41.07%\n","iter 3360: loss 0.0542, time 325.19ms, mfu 41.23%\n","iter 3370: loss 0.0502, time 324.93ms, mfu 41.37%\n","iter 3380: loss 0.0497, time 325.29ms, mfu 41.49%\n","iter 3390: loss 0.0518, time 325.46ms, mfu 41.60%\n","iter 3400: loss 0.0529, time 325.90ms, mfu 41.69%\n","iter 3410: loss 0.0505, time 325.60ms, mfu 41.78%\n","iter 3420: loss 0.0537, time 325.48ms, mfu 41.85%\n","iter 3430: loss 0.0515, time 325.36ms, mfu 41.93%\n","iter 3440: loss 0.0525, time 325.52ms, mfu 41.99%\n","iter 3450: loss 0.0488, time 325.42ms, mfu 42.05%\n","iter 3460: loss 0.0489, time 325.21ms, mfu 42.10%\n","iter 3470: loss 0.0512, time 324.94ms, mfu 42.16%\n","iter 3480: loss 0.0518, time 325.19ms, mfu 42.20%\n","iter 3490: loss 0.0488, time 325.24ms, mfu 42.24%\n","step 3500: train loss 0.0281, val loss 9.1353\n","saving checkpoint to /home/paperspace/llm-engineering-course/hw1/out\n","iter 3500: loss 0.0482, time 6488.14ms, mfu 38.23%\n","iter 3510: loss 0.0499, time 324.67ms, mfu 38.67%\n","iter 3520: loss 0.0476, time 324.12ms, mfu 39.08%\n","iter 3530: loss 0.0466, time 325.18ms, mfu 39.43%\n","iter 3540: loss 0.0449, time 325.15ms, mfu 39.75%\n","iter 3550: loss 0.0473, time 324.79ms, mfu 40.04%\n","iter 3560: loss 0.0467, time 324.72ms, mfu 40.30%\n","iter 3570: loss 0.0459, time 325.50ms, mfu 40.53%\n","iter 3580: loss 0.0447, time 325.50ms, mfu 40.73%\n","iter 3590: loss 0.0482, time 325.05ms, mfu 40.92%\n","iter 3600: loss 0.0468, time 325.72ms, mfu 41.08%\n","iter 3610: loss 0.0476, time 325.89ms, mfu 41.23%\n","iter 3620: loss 0.0459, time 325.47ms, mfu 41.36%\n","iter 3630: loss 0.0440, time 325.65ms, mfu 41.48%\n","iter 3640: loss 0.0439, time 325.70ms, mfu 41.58%\n","iter 3650: loss 0.0446, time 325.22ms, mfu 41.69%\n","iter 3660: loss 0.0463, time 325.73ms, mfu 41.77%\n","iter 3670: loss 0.0439, time 325.56ms, mfu 41.85%\n","iter 3680: loss 0.0461, time 325.68ms, mfu 41.92%\n","iter 3690: loss 0.0458, time 325.35ms, mfu 41.98%\n","iter 3700: loss 0.0425, time 325.50ms, mfu 42.04%\n","iter 3710: loss 0.0443, time 325.36ms, mfu 42.10%\n","iter 3720: loss 0.0433, time 325.51ms, mfu 42.14%\n","iter 3730: loss 0.0421, time 325.13ms, mfu 42.19%\n","iter 3740: loss 0.0434, time 325.38ms, mfu 42.23%\n","step 3750: train loss 0.0270, val loss 9.0152\n","saving checkpoint to /home/paperspace/llm-engineering-course/hw1/out\n","iter 3750: loss 0.0456, time 6387.85ms, mfu 38.22%\n","iter 3760: loss 0.0446, time 324.42ms, mfu 38.67%\n","iter 3770: loss 0.0451, time 324.87ms, mfu 39.07%\n","iter 3780: loss 0.0444, time 324.52ms, mfu 39.43%\n","iter 3790: loss 0.0436, time 324.87ms, mfu 39.75%\n","iter 3800: loss 0.0403, time 325.10ms, mfu 40.04%\n","iter 3810: loss 0.0429, time 325.42ms, mfu 40.29%\n","iter 3820: loss 0.0422, time 325.15ms, mfu 40.52%\n","iter 3830: loss 0.0432, time 325.68ms, mfu 40.72%\n","iter 3840: loss 0.0401, time 325.26ms, mfu 40.91%\n","iter 3850: loss 0.0396, time 324.98ms, mfu 41.08%\n","iter 3860: loss 0.0410, time 325.29ms, mfu 41.23%\n","iter 3870: loss 0.0436, time 325.27ms, mfu 41.37%\n","iter 3880: loss 0.0408, time 324.57ms, mfu 41.50%\n","iter 3890: loss 0.0405, time 325.20ms, mfu 41.61%\n","iter 3900: loss 0.0429, time 324.48ms, mfu 41.72%\n","iter 3910: loss 0.0407, time 325.04ms, mfu 41.81%\n","iter 3920: loss 0.0424, time 325.37ms, mfu 41.89%\n","iter 3930: loss 0.0430, time 324.67ms, mfu 41.97%\n","iter 3940: loss 0.0397, time 325.34ms, mfu 42.03%\n","iter 3950: loss 0.0403, time 324.70ms, mfu 42.09%\n","iter 3960: loss 0.0411, time 325.31ms, mfu 42.14%\n","iter 3970: loss 0.0405, time 325.13ms, mfu 42.19%\n","iter 3980: loss 0.0411, time 325.58ms, mfu 42.22%\n","iter 3990: loss 0.0408, time 325.50ms, mfu 42.26%\n","step 4000: train loss 0.0256, val loss 9.1585\n","saving checkpoint to /home/paperspace/llm-engineering-course/hw1/out\n","iter 4000: loss 0.0383, time 6361.24ms, mfu 38.25%\n","iter 4010: loss 0.0386, time 324.55ms, mfu 38.69%\n","iter 4020: loss 0.0398, time 324.23ms, mfu 39.10%\n","iter 4030: loss 0.0385, time 325.43ms, mfu 39.44%\n","iter 4040: loss 0.0401, time 325.18ms, mfu 39.76%\n","iter 4050: loss 0.0385, time 324.46ms, mfu 40.05%\n","iter 4060: loss 0.0384, time 325.14ms, mfu 40.31%\n","iter 4070: loss 0.0380, time 325.35ms, mfu 40.54%\n","iter 4080: loss 0.0385, time 325.12ms, mfu 40.74%\n","iter 4090: loss 0.0391, time 325.44ms, mfu 40.93%\n","iter 4100: loss 0.0382, time 324.75ms, mfu 41.10%\n","iter 4110: loss 0.0371, time 325.07ms, mfu 41.25%\n","iter 4120: loss 0.0378, time 324.90ms, mfu 41.39%\n","iter 4130: loss 0.0362, time 324.83ms, mfu 41.52%\n","iter 4140: loss 0.0385, time 325.18ms, mfu 41.63%\n","iter 4150: loss 0.0367, time 325.00ms, mfu 41.73%\n","iter 4160: loss 0.0362, time 325.33ms, mfu 41.81%\n","iter 4170: loss 0.0381, time 324.96ms, mfu 41.89%\n","iter 4180: loss 0.0386, time 324.52ms, mfu 41.97%\n","iter 4190: loss 0.0344, time 324.94ms, mfu 42.04%\n","iter 4200: loss 0.0371, time 324.34ms, mfu 42.11%\n","iter 4210: loss 0.0359, time 325.25ms, mfu 42.16%\n","iter 4220: loss 0.0359, time 325.21ms, mfu 42.20%\n","iter 4230: loss 0.0353, time 325.72ms, mfu 42.23%\n","iter 4240: loss 0.0365, time 325.99ms, mfu 42.26%\n","step 4250: train loss 0.0234, val loss 9.1477\n","saving checkpoint to /home/paperspace/llm-engineering-course/hw1/out\n","iter 4250: loss 0.0353, time 6435.97ms, mfu 38.25%\n","iter 4260: loss 0.0365, time 324.66ms, mfu 38.69%\n","iter 4270: loss 0.0355, time 323.87ms, mfu 39.10%\n","iter 4280: loss 0.0366, time 324.05ms, mfu 39.47%\n","iter 4290: loss 0.0370, time 324.82ms, mfu 39.78%\n","iter 4300: loss 0.0371, time 324.72ms, mfu 40.07%\n","iter 4310: loss 0.0364, time 325.34ms, mfu 40.32%\n","iter 4320: loss 0.0350, time 324.93ms, mfu 40.55%\n","iter 4330: loss 0.0358, time 325.08ms, mfu 40.76%\n","iter 4340: loss 0.0363, time 324.91ms, mfu 40.95%\n","iter 4350: loss 0.0350, time 324.71ms, mfu 41.12%\n","iter 4360: loss 0.0337, time 325.45ms, mfu 41.27%\n","iter 4370: loss 0.0345, time 325.76ms, mfu 41.39%\n","iter 4380: loss 0.0362, time 324.70ms, mfu 41.52%\n","iter 4390: loss 0.0352, time 325.46ms, mfu 41.62%\n","iter 4400: loss 0.0358, time 325.22ms, mfu 41.72%\n","iter 4410: loss 0.0353, time 325.07ms, mfu 41.81%\n","iter 4420: loss 0.0349, time 325.19ms, mfu 41.89%\n","iter 4430: loss 0.0326, time 324.62ms, mfu 41.97%\n","iter 4440: loss 0.0333, time 325.13ms, mfu 42.03%\n","iter 4450: loss 0.0348, time 325.34ms, mfu 42.09%\n","iter 4460: loss 0.0334, time 325.18ms, mfu 42.14%\n","iter 4470: loss 0.0349, time 324.88ms, mfu 42.19%\n","iter 4480: loss 0.0322, time 324.97ms, mfu 42.23%\n","iter 4490: loss 0.0359, time 325.70ms, mfu 42.26%\n","step 4500: train loss 0.0227, val loss 9.1699\n","saving checkpoint to /home/paperspace/llm-engineering-course/hw1/out\n","iter 4500: loss 0.0346, time 6530.68ms, mfu 38.25%\n","iter 4510: loss 0.0349, time 324.55ms, mfu 38.69%\n","iter 4520: loss 0.0344, time 325.08ms, mfu 39.09%\n","iter 4530: loss 0.0320, time 324.98ms, mfu 39.44%\n","iter 4540: loss 0.0329, time 324.91ms, mfu 39.76%\n","iter 4550: loss 0.0329, time 324.75ms, mfu 40.05%\n","iter 4560: loss 0.0336, time 325.07ms, mfu 40.31%\n","iter 4570: loss 0.0322, time 324.97ms, mfu 40.54%\n","iter 4580: loss 0.0320, time 324.40ms, mfu 40.76%\n","iter 4590: loss 0.0338, time 325.12ms, mfu 40.94%\n","iter 4600: loss 0.0344, time 325.26ms, mfu 41.11%\n","iter 4610: loss 0.0309, time 324.78ms, mfu 41.26%\n","iter 4620: loss 0.0324, time 324.80ms, mfu 41.40%\n","iter 4630: loss 0.0321, time 325.31ms, mfu 41.52%\n","iter 4640: loss 0.0344, time 325.30ms, mfu 41.63%\n","iter 4650: loss 0.0309, time 325.25ms, mfu 41.72%\n","iter 4660: loss 0.0325, time 325.96ms, mfu 41.80%\n","iter 4670: loss 0.0329, time 325.16ms, mfu 41.88%\n","iter 4680: loss 0.0311, time 324.97ms, mfu 41.96%\n","iter 4690: loss 0.0325, time 325.62ms, mfu 42.02%\n","iter 4700: loss 0.0328, time 325.04ms, mfu 42.08%\n","iter 4710: loss 0.0326, time 325.18ms, mfu 42.13%\n","iter 4720: loss 0.0330, time 325.27ms, mfu 42.18%\n","iter 4730: loss 0.0325, time 325.01ms, mfu 42.22%\n","iter 4740: loss 0.0335, time 325.95ms, mfu 42.25%\n","step 4750: train loss 0.0218, val loss 9.1817\n","saving checkpoint to /home/paperspace/llm-engineering-course/hw1/out\n","iter 4750: loss 0.0317, time 6402.51ms, mfu 38.24%\n","iter 4760: loss 0.0302, time 324.95ms, mfu 38.68%\n","iter 4770: loss 0.0335, time 324.72ms, mfu 39.08%\n","iter 4780: loss 0.0313, time 324.82ms, mfu 39.44%\n","iter 4790: loss 0.0319, time 326.41ms, mfu 39.74%\n","iter 4800: loss 0.0317, time 324.78ms, mfu 40.03%\n","iter 4810: loss 0.0327, time 324.72ms, mfu 40.29%\n","iter 4820: loss 0.0321, time 325.14ms, mfu 40.52%\n","iter 4830: loss 0.0315, time 325.71ms, mfu 40.72%\n","iter 4840: loss 0.0322, time 325.16ms, mfu 40.91%\n","iter 4850: loss 0.0308, time 324.97ms, mfu 41.08%\n","iter 4860: loss 0.0328, time 325.18ms, mfu 41.24%\n","iter 4870: loss 0.0341, time 325.66ms, mfu 41.37%\n","iter 4880: loss 0.0301, time 324.97ms, mfu 41.49%\n","iter 4890: loss 0.0314, time 324.91ms, mfu 41.61%\n","iter 4900: loss 0.0326, time 325.25ms, mfu 41.71%\n","iter 4910: loss 0.0313, time 325.22ms, mfu 41.80%\n","iter 4920: loss 0.0304, time 325.38ms, mfu 41.87%\n","iter 4930: loss 0.0325, time 325.44ms, mfu 41.94%\n","iter 4940: loss 0.0304, time 324.97ms, mfu 42.01%\n","iter 4950: loss 0.0322, time 325.76ms, mfu 42.06%\n","iter 4960: loss 0.0305, time 325.21ms, mfu 42.12%\n","iter 4970: loss 0.0305, time 325.29ms, mfu 42.17%\n","iter 4980: loss 0.0292, time 325.29ms, mfu 42.21%\n","iter 4990: loss 0.0307, time 325.64ms, mfu 42.24%\n","step 5000: train loss 0.0212, val loss 9.2389\n","saving checkpoint to /home/paperspace/llm-engineering-course/hw1/out\n","iter 5000: loss 0.0310, time 6474.66ms, mfu 38.23%\n"]}],"source":["X, Y = get_batch('train')\n","t0 = time.time()\n","local_iter_num = 0\n","raw_model = model\n","running_mfu = -1.0 # model flops utilization\n","\n","while True:\n","    # determine and set the learning rate for this iteration\n","    lr = get_lr(iter_num) if decay_lr else learning_rate\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","\n","    # evaluate the loss on train/val sets and write checkpoints\n","    if iter_num % eval_interval == 0 and master_process:\n","        losses = estimate_loss()\n","        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","        if losses['val'] < best_val_loss or always_save_checkpoint:\n","            best_val_loss = losses['val']\n","            if iter_num > 0:\n","                checkpoint = {\n","                    'model': raw_model.state_dict(),\n","                    'optimizer': optimizer.state_dict(),\n","                    'model_args': model_args,\n","                    'iter_num': iter_num,\n","                    'best_val_loss': best_val_loss,\n","                    'config': config,\n","                }\n","                print(f\"saving checkpoint to {out_dir}\")\n","                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n","    if iter_num == 0 and eval_only:\n","        break\n","\n","    # forward backward update, with optional gradient accumulation to simulate larger batch size\n","    # and using the GradScaler if data type is float16\n","    for micro_step in range(gradient_accumulation_steps):\n","        with ctx:\n","            logits, loss = model(X, Y)\n","            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n","        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n","        X, Y = get_batch('train')\n","        # backward pass, with gradient scaling if training in fp16\n","        scaler.scale(loss).backward()\n","    # clip the gradient\n","    if grad_clip != 0.0:\n","        scaler.unscale_(optimizer)\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n","    # step the optimizer and scaler if training in fp16\n","    scaler.step(optimizer)\n","    scaler.update()\n","    # flush the gradients as soon as we can, no need for this memory anymore\n","    optimizer.zero_grad(set_to_none=True)\n","\n","    # timing and logging\n","    t1 = time.time()\n","    dt = t1 - t0\n","    t0 = t1\n","    if iter_num % log_interval == 0 and master_process:\n","        # get loss as float. note: this is a CPU-GPU sync point\n","        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n","        lossf = loss.item() * gradient_accumulation_steps\n","        if local_iter_num >= 5: # let the training loop settle a bit\n","            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n","            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n","        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n","    iter_num += 1\n","    local_iter_num += 1\n","\n","    # termination conditions\n","    if iter_num > max_iters:\n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["It is overfitting quite a bit.  I tried increasing the batch size and the dropout to\n","no avail.  What other options are there?"]},{"cell_type":"markdown","metadata":{"id":"L2J5JlRxFJOM"},"source":["## Generating Outputs with our New Model\n","\n","Now we can leverage the `sample.py` file to generate outputs from our model!"]},{"cell_type":"markdown","metadata":{"id":"eo_QP1ITFfX2"},"source":["### Generation Set Up and Model Loading"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"-vftqU9LheEK"},"outputs":[],"source":["import os\n","import pickle\n","from contextlib import nullcontext\n","import torch\n","import tiktoken\n","from model import GPTConfig, GPT\n","\n","# -----------------------------------------------------------------------------\n","init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n","out_dir = base_path / 'out' # ignored if init_from is not 'resume'\n","start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n","num_samples = 10 # number of samples to draw\n","max_new_tokens = 500 # number of tokens generated in each sample\n","temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n","top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n","seed = 1337\n","device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n","dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n","compile = False # use PyTorch 2.0 to compile the model to be faster\n","# -----------------------------------------------------------------------------\n","\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n","torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n","device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n","ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n","ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":952,"status":"ok","timestamp":1699400814190,"user":{"displayName":"Chris Alexiuk","userId":"06521420203810986327"},"user_tz":300},"id":"FQRB3j7iiNkl","outputId":"f9efa62a-b0ed-4941-e28f-54280190cb72"},"outputs":[{"name":"stdout","output_type":"stream","text":["number of parameters: 100.49M\n"]}],"source":["# model\n","if init_from == 'resume':\n","    # init from a model saved in a specific directory\n","    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n","    checkpoint = torch.load(ckpt_path, map_location=device)\n","    gptconf = GPTConfig(**checkpoint['model_args'])\n","    model = GPT(gptconf)\n","    state_dict = checkpoint['model']\n","    unwanted_prefix = '_orig_mod.'\n","    for k,v in list(state_dict.items()):\n","        if k.startswith(unwanted_prefix):\n","            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n","    model.load_state_dict(state_dict)"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"N1YAy8DriVZZ"},"outputs":[],"source":["model.eval()\n","model.to(device)\n","if compile:\n","    model = torch.compile(model) # requires PyTorch 2.0 (optional)"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"KoB-5ZuLicAT"},"outputs":[],"source":["from transformers import GPT2Tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained(base_path / \"tokenizer\", unk_token=\"[UNK]\")\n","\n","enc = tokenizer\n","encode = lambda s: enc.encode(s)\n","decode = lambda l: enc.decode(l)"]},{"cell_type":"markdown","metadata":{"id":"mkTQ9wo7FjYU"},"source":["### Generation!"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22734,"status":"ok","timestamp":1699400836923,"user":{"displayName":"Chris Alexiuk","userId":"06521420203810986327"},"user_tz":300},"id":"NmTcaHCjii5l","outputId":"0672880d-f82b-41a5-f8f0-a38578c097d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","Shepherd:\n","None, sir; I have no pheasant, cock nor hen.\n","\n","AUTOLYCUS:\n","How blessed are we that are not simple men!\n","Yet nature might have made me as these are,\n","Therefore I will not disdain.\n","\n","Clown:\n","This cannot be but a great courtier.\n","\n","Shepherd:\n","His garments are rich, but he wears\n","them not handsomely.\n","\n","Clown:\n","He seems to be the more noble in being fantastical:\n","a great man, I'll warrant; I know by the picking\n","on's teeth.\n","\n","AUTOLYCUS:\n","The fardel there? what's i' the fardel?\n","Wherefore that box?\n","\n","Shepherd:\n","Sir, there lies such secrets in this fardel and box,\n","which none must know but the king; and which he\n","shall know within this hour, if I may come to the\n","speech of him.\n","\n","AUTOLYCUS:\n","Age, thou hast lost thy labour.\n","\n","Shepherd:\n","Why, sir?\n","\n","AUTOLYCUS:\n","The king is not at the palace; he is gone aboard a\n","new ship to purge melancholy and air himself: for,\n","if thou beest capable of things serious, thou must\n","know the king is full of grief.\n","\n","Shepard:\n","So 'tis said, sir; about his son, that should have\n","married a shepherd's daughter.\n","\n","AUTOLYCUS:\n","If that shepherd be not in hand-fast, let him fly:\n","the curses he shall have, the tortures he shall\n","feel, will break the back of man, the heart of monster.\n","\n","Clown:\n","Think you so, sir?\n","\n","AUTOLYCUS:\n","Not he alone shall suffer what wit can make heavy\n","and vengeance bitter; but those that are germane to\n","him, though removed fifty times, shall all come\n","under the hangman: which though it be great pity,\n","yet it is necessary. An old sheep-whistling rogue a\n","ram-tender, to offer to have his daughter come into\n","grace! Some say he shall be stoned; but that death\n","is too soft for him, say I draw our throne into a\n","sheep-cote! all deaths are too few, the sharpest too easy.\n","\n","Clown:\n","H\n","---------------\n","\n","O, but not by much I was born so high,\n","Our aery buildeth in the cedar's top,\n","And dallies with the wind and scorns the sun.\n","\n","QUEEN MARGARET:\n","And turns the sun to shade; alas! alas!\n","Witness my son, now in the shade of death;\n","Whose bright out-shining beams thy cloudy wrath\n","Hath in eternal darkness folded up.\n","Your aery buildeth in our aery's nest.\n","O God, that seest it, do not suffer it!\n","As it was won with blood, lost be it so!\n","\n","BUCKINGHAM:\n","Have done! for shame, if not for charity.\n","\n","QUEEN MARGARET:\n","Urge neither charity nor shame to me:\n","Uncharitably with me have you dealt,\n","And shamefully by you my hopes are butcher'd.\n","My charity is outrage, life my shame\n","And in that shame still live my sorrow's rage.\n","\n","BUCKINGHAM:\n","Have done, have done.\n","\n","QUEEN MARGARET:\n","O princely Buckingham I'll kiss thy hand,\n","In sign of league and amity with thee:\n","Now fair befal thee and thy noble house!\n","Thy garments are not spotted with our blood,\n","Nor thou within the compass of my curse.\n","\n","BUCKINGHAM:\n","Nor no one here; for curses never pass\n","The lips of those that breathe them in the air.\n","\n","QUEEN MARGARET:\n","I'll not believe but they ascend the sky,\n","And there awake God's gentle-sleeping peace.\n","O Buckingham, take heed of yonder dog!\n","Look, when he fawns, he bites; and when he bites,\n","His venom tooth will rankle to the death:\n","Have not to do with him, beware of him;\n","Sin, death, and hell have set their marks on him,\n","And all their ministers attend on him.\n","\n","GLOUCESTER:\n","What doth she say, my Lord of Buckingham?\n","\n","BUCKINGHAM:\n","Nothing that I respect, my gracious lord.\n","\n","QUEEN MARGARET:\n","What, dost thou scorn me for my gentle counsel?\n","And soothe the devil that I warn thee from?\n","O, but remember this another day,\n","When he shall split thy very heart with sorrow,\n","And say poor Margaret was a prophetess\n","---------------\n","\n","Or else it would have gall'd his surly nature,\n","Which easily endures not article\n","Tying him to aught; so putting him to rage,\n","You should have ta'en the advantage of his choler\n","And pass'd him unelected.\n","\n","BRUTUS:\n","Did you perceive\n","He did solicit you in free contempt\n","When he did need your loves, and do you think\n","That his contempt shall not be bruising to you,\n","When he hath power to crush? Why, had your bodies\n","No heart among you? or had you tongues to cry\n","Against the rectorship of judgment?\n","\n","SICINIUS:\n","Have you\n","Ere now denied the asker? and now again\n","Of him that did not ask, but mock, bestow\n","Your sued-for tongues?\n","\n","Third Citizen:\n","He's not confirm'd; we may deny him yet.\n","\n","Second Citizen:\n","And will deny him:\n","I'll have five hundred voices of that sound.\n","\n","First Citizen:\n","I twice five hundred and their friends to piece 'em.\n","\n","BRUTUS:\n","Get you hence instantly, and tell those friends,\n","They have chose a consul that will from them take\n","Their liberties; make them of no more voice\n","Than dogs that are as often beat for barking\n","As therefore kept to do so.\n","\n","SICINIUS:\n","Let them assemble,\n","And on a safer judgment all revoke\n","Your ignorant election; enforce his pride,\n","And his old hate unto you; besides, forget not\n","With what contempt he wore the humble weed,\n","How in his suit he scorn'd you; but your loves,\n","Thinking upon his services, took from you\n","The apprehension of his present portance,\n","Which most gibingly, ungravely, he did fashion\n","After the inveterate hate he bears you.\n","\n","BRUTUS:\n","Lay\n","A fault on us, your tribunes; that we laboured,\n","No impediment between, but that you must\n","Cast your election on him.\n","\n","SICINIUS:\n","Say, you chose him\n","More after our commandment than as guided\n","By your own true affections, and that your minds,\n","Preoccupied with what you rather must do\n","Than what you should, made you against the grain\n","To voice him consul: lay the fault on\n","---------------\n","\n","My soul's love, my humble thanks, my prayers;\n","That love which virtue begs and virtue grants.\n","\n","KING EDWARD IV:\n","No, by my troth, I did not mean such love.\n","\n","LADY GREY:\n","Why, then you mean not as I thought you did.\n","\n","KING EDWARD IV:\n","But now you partly may perceive my mind.\n","\n","LADY GREY:\n","My mind will never grant what I perceive\n","Your highness aims at, if I aim aright.\n","\n","KING EDWARD IV:\n","To tell thee plain, I aim to lie with thee.\n","\n","LADY GREY:\n","To tell you plain, I had rather lie in prison.\n","\n","KING EDWARD IV:\n","Why, then thou shalt not have thy husband's lands.\n","\n","LADY GREY:\n","Why, then mine honesty shall be my dower;\n","For by that loss I will not purchase them.\n","\n","KING EDWARD IV:\n","Therein thou wrong'st thy children mightily.\n","\n","LADY GREY:\n","Herein your highness wrongs both them and me.\n","But, mighty lord, this merry inclination\n","Accords not with the sadness of my suit:\n","Please you dismiss me either with 'ay' or 'no.'\n","\n","KING EDWARD IV:\n","Ay, if thou wilt say 'ay' to my request;\n","No if thou dost say 'no' to my demand.\n","\n","LADY GREY:\n","Then, no, my lord. My suit is at an end.\n","\n","GLOUCESTER:\n","\n","CLARENCE:\n","\n","KING EDWARD IV:\n","\n","LADY GREY:\n","'Tis better said than done, my gracious lord:\n","I am a subject fit to jest withal,\n","But far unfit to be a sovereign.\n","\n","KING EDWARD IV:\n","Sweet widow, by my state I swear to thee\n","I speak no more than what my soul intends;\n","And that is, to enjoy thee for my love.\n","\n","LADY GREY:\n","And that is more than I will yield unto:\n","I know I am too mean to be your queen,\n","And yet too good to be your concubine.\n","\n","KING EDWARD IV:\n","You cavil, widow: I did mean, my queen.\n","\n","LADY GREY:\n","'Twill grieve your grace my sons should\n","---------------\n","\n","AUFIDIUS:\n","I dare be sworn you were:\n","And, sir, it is no little thing to make\n","Mine eyes to sweat compassion. But, good sir,\n","What peace you'll make, advise me: for my part,\n","I'll not to Rome, I'll back with you; and pray you,\n","Stand to me in this cause. O mother! wife!\n","\n","AUFIDIUS:\n","\n","CORIOLANUS:\n","Ay, by and by;\n","But we will drink together; and you shall bear\n","A better witness back than words, which we,\n","On like conditions, will have counter-seal'd.\n","Come, enter with us. Ladies, you deserve\n","To have a temple built you: all the swords\n","In Italy, and her confederate arms,\n","Could not have made this peace.\n","\n","MENENIUS:\n","See you yond coign o' the Capitol, yond\n","corner-stone?\n","\n","SICINIUS:\n","Why, what of that?\n","\n","MENENIUS:\n","If it be possible for you to displace it with your\n","little finger, there is some hope the ladies of\n","Rome, especially his mother, may prevail with him.\n","But I say there is no hope in't: our throats are\n","sentenced and stay upon execution.\n","\n","SICINIUS:\n","Is't possible that so short a time can alter the\n","condition of a man!\n","\n","MENENIUS:\n","There is differency between a grub and a butterfly;\n","yet your butterfly was a grub. This Marcius is grown\n","from man to dragon: he has wings; he's more than a\n","creeping thing.\n","\n","SICINIUS:\n","He loved his mother dearly.\n","\n","MENENIUS:\n","So did he me: and he no more remembers his mother\n","now than an eight-year-old horse. The tartness\n","of his face sours ripe grapes: when he walks, he\n","moves like an engine, and the ground shrinks before\n","his treading: he is able to pierce a corslet with\n","his eye; talks like a knell, and his hum is a\n","battery. He sits in his state, as a thing made for\n","Alexander. What he bids be done is finished with\n","his bidding. He wants nothing of a god but eternity\n","and a heaven to throne in.\n","\n","---------------\n","\n","And he that got it, sentenced; a young man\n","More fit to do another such offence\n","Than die for this.\n","\n","DUKE VINCENTIO:\n","When must he die?\n","\n","Provost:\n","As I do think, to-morrow.\n","I have provided for you: stay awhile,\n","And you shall be conducted.\n","\n","DUKE VINCENTIO:\n","Repent you, fair one, of the sin you carry?\n","\n","JULIET:\n","I do; and bear the shame most patiently.\n","\n","DUKE VINCENTIO:\n","I'll teach you how you shall arraign your conscience,\n","And try your penitence, if it be sound,\n","Or hollowly put on.\n","\n","JULIET:\n","I'll gladly learn.\n","\n","DUKE VINCENTIO:\n","Love you the man that wrong'd you?\n","\n","JULIET:\n","Yes, as I love the woman that wrong'd him.\n","\n","DUKE VINCENTIO:\n","So then it seems your most offenceful act\n","Was mutually committed?\n","\n","JULIET:\n","Mutually.\n","\n","DUKE VINCENTIO:\n","Then was your sin of heavier kind than his.\n","\n","JULIET:\n","I do confess it, and repent it, father.\n","\n","DUKE VINCENTIO:\n","'Tis meet so, daughter: but lest you do repent,\n","As that the sin hath brought you to this shame,\n","Which sorrow is always towards ourselves, not heaven,\n","Showing we would not spare heaven as we love it,\n","But as we stand in fear,--\n","\n","JULIET:\n","I do repent me, as it is an evil,\n","And take the shame with joy.\n","\n","DUKE VINCENTIO:\n","There rest.\n","Your partner, as I hear, must die to-morrow,\n","And I am going with instruction to him.\n","Grace go with you, Benedicite!\n","\n","JULIET:\n","Must die to-morrow! O injurious love,\n","That respites me a life, whose very comfort\n","Is still a dying horror!\n","\n","Provost:\n","'Tis pity of him.\n","\n","ANGELO:\n","When I would pray and think, I think and pray\n","To several subjects. Heaven hath my empty words;\n","Whilst my invention, hearing not my tongue,\n","Anchors on Isabel: Heaven in\n","---------------\n","\n","CLIFFORD:\n","In dreadful war mayst thou be overcome,\n","Or live in peace abandon'd and despised!\n","\n","WARWICK:\n","Turn this way, Henry, and regard them not.\n","\n","EXETER:\n","They seek revenge and therefore will not yield.\n","\n","KING HENRY VI:\n","Ah, Exeter!\n","\n","WARWICK:\n","Why should you sigh, my lord?\n","\n","KING HENRY VI:\n","Not for myself, Lord Warwick, but my son,\n","Whom I unnaturally shall disinherit.\n","But be it as it may: I here entail\n","The crown to thee and to thine heirs for ever;\n","Conditionally, that here thou take an oath\n","To cease this civil war, and, whilst I live,\n","To honour me as thy king and sovereign,\n","And neither by treason nor hostility\n","To seek to put me down and reign thyself.\n","\n","YORK:\n","This oath I willingly take and will perform.\n","\n","WARWICK:\n","Long live King Henry! Plantagenet embrace him.\n","\n","KING HENRY VI:\n","And long live thou and these thy forward sons!\n","\n","YORK:\n","Now York and Lancaster are reconciled.\n","\n","EXETER:\n","Accursed be he that seeks to make them foes!\n","\n","YORK:\n","Farewell, my gracious lord; I'll to my castle.\n","\n","WARWICK:\n","And I'll keep London with my soldiers.\n","\n","NORFOLK:\n","And I to Norfolk with my followers.\n","\n","MONTAGUE:\n","And I unto the sea from whence I came.\n","\n","KING HENRY VI:\n","And I, with grief and sorrow, to the court.\n","\n","EXETER:\n","Here comes the queen, whose looks bewray her anger:\n","I'll steal away.\n","\n","KING HENRY VI:\n","Exeter, so will I.\n","\n","QUEEN MARGARET:\n","Nay, go not from me; I will follow thee.\n","\n","KING HENRY VI:\n","Be patient, gentle queen, and I will stay.\n","\n","QUEEN MARGARET:\n","Who can be patient in such extremes?\n","Ah, wretched man! would I had died a maid\n","And never seen thee, never borne thee son,\n","Seeing thou hast proved so unnatural a father\n","Hath\n","---------------\n","\n","\n","KING RICHARD III:\n","Why, madam, so you must I have the business.\n","\n","QUEEN ELIZABETH:\n","My lords?\n","\n","KING RICHARD III:\n","Send out a pursuivant at arms\n","To Stanley's regiment; bid him bring his power\n","Before sunrising, lest his son George fall\n","Into the blind cave of eternal night.\n","Fill me a bowl of wine. Give me a watch.\n","Saddle white Surrey for the field to-morrow.\n","Look that my staves be sound, and not too heavy.\n","Ratcliff!\n","\n","RATCLIFF:\n","My lord?\n","\n","KING RICHARD III:\n","Saw'st thou the melancholy Lord Northumberland?\n","\n","RATCLIFF:\n","Thomas the Earl of Surrey, and himself,\n","Much about cock-shut time, from troop to troop\n","Went through the army, cheering up the soldiers.\n","\n","KING RICHARD III:\n","So, I am satisfied. Give me a bowl of wine:\n","I have not that alacrity of spirit,\n","Nor cheer of mind, that I was wont to have.\n","Set it down. Is ink and paper ready?\n","\n","RATCLIFF:\n","It is, my lord.\n","\n","KING RICHARD III:\n","Bid my guard watch; leave me.\n","Ratcliff, about the mid of night come to my tent\n","And help to arm me. Leave me, I say.\n","\n","DERBY:\n","Fortune and victory sit on thy helm!\n","\n","RICHMOND:\n","All comfort that the dark night can afford\n","Be to thy person, noble father-in-law!\n","Tell me, how fares our loving mother?\n","\n","DERBY:\n","I, by attorney, bless thee from thy mother\n","Who prays continually for Richmond's good:\n","So much for that. The silent hours steal on,\n","And flaky darkness breaks within the east.\n","In brief,--for so the season bids us be,--\n","Prepare thy battle early in the morning,\n","And put thy fortune to the arbitrement\n","Of bloody strokes and mortal-staring war.\n","I, as I may--that which I would I cannot,--\n","With best advantage will deceive the time,\n","And aid thee in this doubtful shock of arms:\n","But on thy side I may not be too forward\n","Lest\n","---------------\n","\n","I am a king, and privileged to speak.\n","\n","CLIFFORD:\n","My liege, the wound that bred this meeting here\n","Cannot be cured by words; therefore be still.\n","\n","RICHARD:\n","Then, executioner, unsheathe thy sword:\n","By him that made us all, I am resolved\n","that Clifford's manhood lies upon his tongue.\n","\n","EDWARD:\n","Say, Henry, shall I have my right, or no?\n","A thousand men have broke their fasts to-day,\n","That ne'er shall dine unless thou yield the crown.\n","\n","WARWICK:\n","If thou deny, their blood upon thy head;\n","For York in justice puts his armour on.\n","\n","PRINCE EDWARD:\n","If that be right which Warwick says is right,\n","There is no wrong, but every thing is right.\n","\n","RICHARD:\n","Whoever got thee, there thy mother stands;\n","For, well I wot, thou hast thy mother's tongue.\n","\n","QUEEN MARGARET:\n","But thou art neither like thy sire nor dam;\n","But like a foul mis-shapen stigmatic,\n","Mark'd by the destinies to be avoided,\n","As venom toads, or lizards' dreadful stings.\n","\n","RICHARD:\n","Iron of Naples hid with English gilt,\n","Whose father bears the title of a king,--\n","As if a channel should be call'd the sea,--\n","Shamest thou not, knowing whence thou art extraught,\n","To let thy tongue detect thy base-born heart?\n","\n","EDWARD:\n","A wisp of straw were worth a thousand crowns,\n","To make this shameless callet know herself.\n","Helen of Greece was fairer far than thou,\n","Although thy husband may be Menelaus;\n","And ne'er was Agamemnon's brother wrong'd\n","By that false woman, as this king by thee.\n","His father revell'd in the heart of France,\n","And tamed the king, and made the dauphin stoop;\n","And had he match'd according to his state,\n","He might have kept that glory to this day;\n","But when he took a beggar to his bed,\n","And graced thy poor sire with his bridal-day,\n","Even then that sunshine brew'd a shower for him,\n","That wash'd his father's fortunes forth of France,\n","\n","---------------\n","\n","SICINIUS:\n","This is clean kam.\n","\n","BRUTUS:\n","Merely awry: when he did love his country,\n","It honour'd him.\n","\n","MENENIUS:\n","The service of the foot\n","Being once gangrened, is not then respected\n","For what before it was.\n","\n","BRUTUS:\n","We'll hear no more.\n","Pursue him to his house, and pluck him thence:\n","Lest his infection, being of catching nature,\n","Spread further.\n","\n","MENENIUS:\n","One word more, one word.\n","This tiger-footed rage, when it shall find\n","The harm of unscann'd swiftness, will too late\n","Tie leaden pounds to's heels. Proceed by process;\n","Lest parties, as he is beloved, break out,\n","And sack great Rome with Romans.\n","\n","BRUTUS:\n","If it were so,--\n","\n","SICINIUS:\n","What do ye talk?\n","Have we not had a taste of his obedience?\n","Our aediles smote? ourselves resisted? Come.\n","\n","MENENIUS:\n","Consider this: he has been bred i' the wars\n","Since he could draw a sword, and is ill school'd\n","In bolted language; meal and bran together\n","He throws without distinction. Give me leave,\n","I'll go to him, and undertake to bring him\n","Where he shall answer, by a lawful form,\n","In peace, to his utmost peril.\n","\n","First Senator:\n","Noble tribunes,\n","It is the humane way: the other course\n","Will prove too bloody, and the end of it\n","Unknown to the beginning.\n","\n","SICINIUS:\n","Noble Menenius,\n","Be you then as the people's officer.\n","Masters, lay down your weapons.\n","\n","BRUTUS:\n","Go not home.\n","\n","SICINIUS:\n","Meet on the market-place. We'll attend you there:\n","Where, if you bring not Marcius, we'll proceed\n","In our first way.\n","\n","MENENIUS:\n","I'll bring him to you.\n","Let me desire your company: he must come,\n","Or what is worst will follow.\n","\n","First Senator:\n","Pray you, let's to him.\n","\n","CORIOLANUS:\n","Let them puff all about mine ears, present me\n","Death on the wheel or at\n","---------------\n"]}],"source":["# encode the beginning of the prompt\n","if start.startswith('FILE:'):\n","    with open(start[5:], 'r', encoding='utf-8') as f:\n","        start = f.read()\n","start_ids = encode(start)\n","x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n","\n","# run generation\n","with torch.no_grad():\n","    with ctx:\n","        for k in range(num_samples):\n","            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n","            print(decode(y[0].tolist()))\n","            print('---------------')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["eo_QP1ITFfX2"],"gpuType":"V100","machine_shape":"hm","provenance":[{"file_id":"1W499zNqDRtbXD6wCzeKCGLqPM_DsDyEk","timestamp":1699631343741},{"file_id":"1FgUKk5TNcwTe2J4GJdTF04nVKFXKFlP5","timestamp":1699400865719}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
